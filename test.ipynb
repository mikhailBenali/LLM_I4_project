{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f9ae-8932-4f97-b23c-ce6df94c4421",
   "metadata": {},
   "source": [
    "## Test du modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e092c15-9e28-428f-85b2-c5e1c5ab8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: Hello, how are you?\n",
      "FR: Bonjour, c'est-à-dire?\n",
      "EN: This project is about automatic translation.\n",
      "FR: Cette projet concerne la traduction automatique.\n",
      "EN: The weather is nice today.\n",
      "FR: Le temps est bon aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def translate_base(text):\n",
    "    inputs = base_tokenizer(INSTRUCTION + text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = base_model.generate(\n",
    "        **inputs,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=64,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    return base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "for s in [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]:\n",
    "    print(\"EN:\", s)\n",
    "    print(\"FR:\", translate_base(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e6434-1fcb-4810-9a7c-46782fb9850e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7628a866-c0b7-426c-b766-c12d8e1e3bdd",
   "metadata": {},
   "source": [
    "## Ce qu’il ne faut pas faire pour le modèle T5\n",
    "\n",
    "- Pas de lemmatisation / stemming : tu casserais la forme exacte que le modèle attend.\n",
    "\n",
    "- Pas de suppression de stopwords (“le, de, et…”) : les Transformers utilisent ces mots pour comprendre la syntaxe.\n",
    "\n",
    "- Pas de passage forcé en minuscules si le modèle a été pré-entraîné en respectant la casse.\n",
    "\n",
    "- Pas de suppression massive de ponctuation (les modèles utilisent “?”, “.”, “,”, “:” pour le sens et la segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "**OPUS et Europarl**\n",
    "\n",
    "**OPUS-100 en-fr** : mélange de sources, phrases souvent plus courtes, variées (titres, sous-phrases, etc.).\n",
    "\n",
    "**Europarl en-fr** : phrases plus longues, style plus formel, discours parlementaires → plus de contexte par phrase.\n",
    "\n",
    "Du point de vue du modèle :\n",
    "\n",
    "- OPUS -> donne de la diversité (beaucoup de styles, domaines).\n",
    "\n",
    "- Europarl ->  apprend à gérer des phrases longues / syntaxe compliquée (subordonnées, tournures formelles) => **augmenter** ***max_length*** pour moins tronquer les phrases\n",
    "\n",
    "\n",
    "**Ajouter les TED Talks plus tard pour le langage oral, plus “conversational”, avec un ton explicatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a070bbb-6532-4155-a50f-f2a75d14be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Paramètres de décodage par défaut pour .generate()\n",
    "    # pendant l’éval (puisque predict_with_generate=True), les traductions seront générées avec ces paramètres -> métriques qui reflètent ce décodage\n",
    "    # pendant l’inférence manuelle (model.generate(...) sans redonner ces arguments), ce seront les valeurs par défaut.\n",
    "    \n",
    "    model.config.num_beams = 4              # beam search\n",
    "    model.config.length_penalty = 0.9       # < 1 = un peu plus court, > 1 = plus long\n",
    "    model.config.no_repeat_ngram_size = 3   # évite de répéter des 3-grammes\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data_old(sample_size_train=10000, sample_size_val=2000):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val   = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds   = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def load_data(sample_size_train=10000, sample_size_val=300):\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val   = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "    \n",
    "    return euro_train, euro_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "EN : Mr President, Commissioner, allow me to begin with a word to my honourable friend: Georg, I was delighted to hear your sentence about the small sea ports.\n",
      "FR : Monsieur le Président, Monsieur le Commissaire, Georg, j'ai beaucoup apprécié ta phrase sur les petits ports de mer.\n",
      "\n",
      "--- val example 1 ---\n",
      "EN : Europe and its partners, allied in this difficult military action, have clearly laid down their demands.\n",
      "FR : L'Europe et ses partenaires, alliés dans une action militaire difficile, ont clairement fixé leurs exigences.\n",
      "\n",
      "--- val example 2 ---\n",
      "EN : We must learn from disasters of this kind - modern man must learn from them - that not everything can be controlled but that everything can be limited if adequate precautions are taken.\n",
      "FR : De ce type de catastrophe nous devons tirer comme leçon - l'homme moderne doit en tirer comme leçon - qu'il n'est pas possible de tout contrôler, mais que tout peut être limité à condition de prendre des précautions adéquates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données brutes\n",
    "train_ds, val_ds = load_data()\n",
    "\n",
    "def show_raw_example(ds, idx=0, prefix=\"train\"):\n",
    "    ex = ds[idx]\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"EN :\", ex[\"translation\"][\"en\"])\n",
    "    print(\"FR :\", ex[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "# 3 exemples du train et 3 de la val\n",
    "for i in range(3):\n",
    "    show_raw_example(train_ds, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_raw_example(val_ds, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # Encodage des entrées\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encodage des cibles\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # ignorer les PAD dans la loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ffe0be-9733-4bcd-b503-e03520f3c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 248,020,224 || trainable%: 0.1784\n"
     ]
    }
   ],
   "source": [
    "# Charger modèle & tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "# Re-tokeniser\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f5ed41-0751-4860-aabb-b62d122c6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_example(raw_ds, tokenized_ds, idx=0, prefix=\"train\"):\n",
    "    raw = raw_ds[idx]\n",
    "    tok = tokenized_ds[idx]\n",
    "\n",
    "    print(f\"=== {prefix} example {idx} ===\")\n",
    "    print(\"RAW EN :\", raw[\"translation\"][\"en\"])\n",
    "    print(\"RAW FR :\", raw[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "    # Inputs\n",
    "    print(\"input_ids[:20] :\", tok[\"input_ids\"][:20])\n",
    "    print(\"Decoded input  :\", tokenizer.decode(tok[\"input_ids\"], skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "    # Labels (enlevant les -100 pour re-décoder)\n",
    "    labels = tok[\"labels\"]\n",
    "    # On remplace les -100 par pad_token_id pour pouvoir décoder\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels_for_decode = [pad_id if x == -100 else x for x in labels]\n",
    "    print(\"labels[:20]     :\", labels[:20])\n",
    "    print(\"Decoded labels  :\", tokenizer.decode(labels_for_decode, skip_special_tokens=True))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1ad48a-590c-4800-b266-50f560637202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train example 0 ===\n",
      "RAW EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "RAW FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 282, 1611, 7, 6, 28, 69, 351, 6, 69, 1543, 13, 3065, 11, 69]\n",
      "Decoded input  : Translate English to French: As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "\n",
      "labels[:20]     : [622, 2430, 3890, 35, 7, 238, 678, 7056, 6, 393, 1089, 11183, 6, 1089, 1543, 20, 50, 25060, 3, 15]\n",
      "Decoded labels  : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "=== train example 1 ===\n",
      "RAW EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "RAW FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 94, 405, 5071, 2516, 21, 1402, 24, 273, 113, 33, 300, 8, 3, 22469]\n",
      "Decoded input  : Translate English to French: It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "\n",
      "labels[:20]     : [802, 259, 3, 26, 31, 6752, 29500, 238, 110, 3787, 285, 21442, 29, 17, 3, 8934, 20, 50, 953, 20]\n",
      "Decoded labels  : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "=== train example 2 ===\n",
      "RAW EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "RAW FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 101, 17516, 857, 24, 8, 12326, 19, 1345, 1879, 250, 34, 11708, 7, 3]\n",
      "Decoded input  : Translate English to French: We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "\n",
      "labels[:20]     : [3, 18990, 6, 678, 12182, 106, 7, 238, 90, 29296, 259, 247, 3, 40, 22, 7408, 19185, 7, 288, 6]\n",
      "Decoded labels  : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "=== val example 0 ===\n",
      "RAW EN : Mr President, Commissioner, allow me to begin with a word to my honourable friend: Georg, I was delighted to hear your sentence about the small sea ports.\n",
      "RAW FR : Monsieur le Président, Monsieur le Commissaire, Georg, j'ai beaucoup apprécié ta phrase sur les petits ports de mer.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 1363, 1661, 6, 14595, 6, 995, 140, 12, 1731, 28, 3, 9, 1448, 12]\n",
      "Decoded input  : Translate English to French: Mr President, Commissioner, allow me to begin with a word to my honourable friend: Georg, I was delighted to hear your sentence about the small sea ports.\n",
      "\n",
      "labels[:20]     : [24517, 90, 22971, 6, 24517, 90, 2570, 11502, 2378, 6, 20916, 6, 3, 354, 31, 9, 23, 3933, 26377, 3]\n",
      "Decoded labels  : Monsieur le Président, Monsieur le Commissaire, Georg, j'ai beaucoup apprécié ta phrase sur les petits ports de mer.\n",
      "\n",
      "=== val example 1 ===\n",
      "RAW EN : Europe and its partners, allied in this difficult military action, have clearly laid down their demands.\n",
      "RAW FR : L'Europe et ses partenaires, alliés dans une action militaire difficile, ont clairement fixé leurs exigences.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 1740, 11, 165, 3222, 6, 3, 26655, 16, 48, 1256, 2716, 1041, 6, 43]\n",
      "Decoded input  : Translate English to French: Europe and its partners, allied in this difficult military action, have clearly laid down their demands.\n",
      "\n",
      "labels[:20]     : [301, 31, 11351, 3, 15, 17, 1394, 17876, 6, 491, 22263, 247, 245, 1041, 27034, 11166, 6, 30, 17, 3]\n",
      "Decoded labels  : L'Europe et ses partenaires, alliés dans une action militaire difficile, ont clairement fixé leurs exigences.\n",
      "\n",
      "=== val example 2 ===\n",
      "RAW EN : We must learn from disasters of this kind - modern man must learn from them - that not everything can be controlled but that everything can be limited if adequate precautions are taken.\n",
      "RAW FR : De ce type de catastrophe nous devons tirer comme leçon - l'homme moderne doit en tirer comme leçon - qu'il n'est pas possible de tout contrôler, mais que tout peut être limité à condition de prendre des précautions adéquates.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 101, 398, 669, 45, 6912, 7, 13, 48, 773, 3, 18, 941, 388, 398]\n",
      "Decoded input  : Translate English to French: We must learn from disasters of this kind - modern man must learn from them - that not everything can be controlled but that everything can be limited if adequate precautions are taken.\n",
      "\n",
      "labels[:20]     : [374, 197, 686, 20, 25334, 678, 20, 5951, 7, 3, 25061, 1011, 90, 14163, 3, 18, 3, 40, 31, 6814]\n",
      "Decoded labels  : De ce type de catastrophe nous devons tirer comme leçon - l'homme moderne doit en tirer comme leçon - qu'il n'est pas possible de tout contrôler, mais que tout peut être limité à condition de prendre des précautions adéquates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_processed_example(train_ds, tokenized_train, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_processed_example(val_ds, tokenized_val, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    # eval_preds peut être un tuple (preds, labels)\n",
    "    # ou un objet EvalPrediction avec .predictions et .label_ids\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement le 1er élément\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # On met tout en np.array pour être tranquille\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    # Cas où preds = logits (batch, seq_len, vocab_size) -> on prend l'argmax\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # On s'assure que ce sont bien des entiers\n",
    "    preds = preds.astype(\"int64\")\n",
    "\n",
    "    # si jamais il y a des valeurs négatives dans preds on les remplace par pad_token_id avant decode\n",
    "    preds[preds < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Gestion des labels : on remet pad_token_id à la place des -100 pour décoder\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode des prédictions et des labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33188-476e-43b6-81a8-34394241864b",
   "metadata": {},
   "source": [
    "## Métriques sur modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b37d9e-9dea-4adc-8bbf-5e6ce0783d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_19976\\3993741208.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3933814764022827, 'eval_model_preparation_time': 0.003, 'eval_bleu': 22.80254763232047, 'eval_meteor': 0.4819317956188341, 'eval_rouge1': 0.5556213377900112, 'eval_rouge2': 0.34619395196218433, 'eval_rougeL': 0.5093467802112331, 'eval_runtime': 44.9613, 'eval_samples_per_second': 6.672, 'eval_steps_per_second': 0.845}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "BASE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train_base, tokenized_val_base = tokenize_datasets(train_ds, val_ds, tokenizer_base)\n",
    "\n",
    "\n",
    "data_collator_base = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_base,\n",
    "    model=model_base,\n",
    ")\n",
    "\n",
    "eval_args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"baseline_flan_t5_en_fr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    # paramètres de génération cohérents avec ceux du fine-tuning\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"logs_baseline\",\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_base,\n",
    "    args=eval_args_base,\n",
    "    eval_dataset=tokenized_val_base,\n",
    "    tokenizer=tokenizer_base,\n",
    "    data_collator=data_collator_base,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer_base),\n",
    ")\n",
    "\n",
    "baseline_metrics = trainer_base.evaluate()\n",
    "print(baseline_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_flan_t5_en_fr\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",  \n",
    "        greater_is_better=False,       \n",
    "        logging_steps=50,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()  \n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarde du modèle LoRA + tokenizer dans le même dossier\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60fee12-2491-42a8-aecc-beb8a9ac746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "\n",
    "    input_text = INSTRUCTION + sentence\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)  # les tensors vont sur le même device que le modèle\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca642d21-53be-46d4-a157-e3e6337e00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 248,020,224 || trainable%: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_19976\\2447461640.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2501' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2501/3750 06:47 < 03:23, 6.14 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.760900</td>\n",
       "      <td>1.401531</td>\n",
       "      <td>6.097579</td>\n",
       "      <td>0.269392</td>\n",
       "      <td>0.389102</td>\n",
       "      <td>0.225535</td>\n",
       "      <td>0.358436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7/38 00:02 < 00:12, 2.40 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    train_ds, val_ds = load_data()\n",
    "    tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "    trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca0fd8-ae8d-479f-bde0-acce1fb23652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "PEFT_DIR = \"./finetuned_flan_t5_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Lire la config PEFT (LoRA) depuis le dossier local\n",
    "peft_config = PeftConfig.from_pretrained(PEFT_DIR, local_files_only=True)\n",
    "\n",
    "# 2) Charger le modèle de base utilisé pendant le fine-tuning\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "\n",
    "# 3) Appliquer les poids LoRA entraînés\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 4) Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "print(\"Modèle LoRA + tokenizer rechargés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85a61a-431c-43ae-84b6-0f42ebda2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Quelques phrases du dataset train\n",
    "for i in range(3):\n",
    "    en = train_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = train_ds[i][\"translation\"][\"fr\"]\n",
    "    fr_pred = translate_sentence(en, model, tokenizer)\n",
    "    print(\"=== Exemple train\", i, \"===\")\n",
    "    print(\"EN     :\", en)\n",
    "    print(\"FR gold:\", fr_gold)\n",
    "    print(\"FR pred:\", fr_pred)\n",
    "    print()\n",
    "\n",
    "# 2) phrases de test perso\n",
    "tests = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"EN :\", s)\n",
    "    print(\"FR :\", translate_sentence(s, model, tokenizer))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b53398-1fdb-49b0-907c-fc4c86aac63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c99d7d-f98d-4911-bfc7-ed5755160283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate_sentence(\"This is a small translation test.\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381ba35-2591-4787-88c1-f773526a1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

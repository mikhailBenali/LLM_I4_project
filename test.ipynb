{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb6a9f41-64d6-4976-a9fc-b9e00aa3c7f9",
   "metadata": {},
   "source": [
    "## Membres du Groupe de Projet\n",
    "\n",
    "- Maxence KAMIONKA\n",
    "- Mikhaïl BENALI\n",
    "- Hadja BAH\n",
    "- Emmanuel DAGNOGO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd78f78-71ef-42b0-ac1c-869ca9b16332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f9ae-8932-4f97-b23c-ce6df94c4421",
   "metadata": {},
   "source": [
    "## Test du modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e092c15-9e28-428f-85b2-c5e1c5ab8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: Hello, how are you?\n",
      "FR: Bonjour, c'est-à-dire?\n",
      "EN: This project is about automatic translation.\n",
      "FR: Cette projet concerne la traduction automatique.\n",
      "EN: The weather is nice today.\n",
      "FR: Le temps est bon aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def translate_base(text):\n",
    "    inputs = base_tokenizer(INSTRUCTION + text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = base_model.generate(\n",
    "        **inputs,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=128,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    return base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "for s in [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]:\n",
    "    print(\"EN:\", s)\n",
    "    print(\"FR:\", translate_base(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e6434-1fcb-4810-9a7c-46782fb9850e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7628a866-c0b7-426c-b766-c12d8e1e3bdd",
   "metadata": {},
   "source": [
    "## Ce qu’il ne faut pas faire pour le modèle T5\n",
    "\n",
    "- Pas de lemmatisation / stemming : tu casserais la forme exacte que le modèle attend.\n",
    "\n",
    "- Pas de suppression de stopwords (“le, de, et…”) : les Transformers utilisent ces mots pour comprendre la syntaxe.\n",
    "\n",
    "- Pas de passage forcé en minuscules si le modèle a été pré-entraîné en respectant la casse.\n",
    "\n",
    "- Pas de suppression massive de ponctuation (les modèles utilisent “?”, “.”, “,”, “:” pour le sens et la segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "**OPUS et Europarl**\n",
    "\n",
    "**OPUS-100 en-fr** : mélange de sources, phrases souvent plus courtes, variées (titres, sous-phrases, etc.).\n",
    "\n",
    "**Europarl en-fr** : phrases plus longues, style plus formel, discours parlementaires → plus de contexte par phrase.\n",
    "\n",
    "Du point de vue du modèle :\n",
    "\n",
    "- OPUS -> donne de la diversité (beaucoup de styles, domaines).\n",
    "\n",
    "- Europarl ->  apprend à gérer des phrases longues / syntaxe compliquée (subordonnées, tournures formelles) => **augmenter** ***max_length*** pour moins tronquer les phrases\n",
    "\n",
    "\n",
    "**Ajouter les TED Talks plus tard pour le langage oral, plus “conversational”, avec un ton explicatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a070bbb-6532-4155-a50f-f2a75d14be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Paramètres de décodage par défaut pour .generate()\n",
    "    # pendant l’éval (puisque predict_with_generate=True), les traductions seront générées avec ces paramètres -> métriques qui reflètent ce décodage\n",
    "    # pendant l’inférence manuelle (model.generate(...) sans redonner ces arguments), ce seront les valeurs par défaut.\n",
    "    \n",
    "    model.config.num_beams = 4              # beam search\n",
    "    model.config.length_penalty = 0.9       # < 1 = un peu plus court, > 1 = plus long\n",
    "    model.config.no_repeat_ngram_size = 3   # évite de répéter des 3-grammes\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data_old(sample_size_train=10000, sample_size_val=2000):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def load_data(sample_size_train=80000, sample_size_val=800):\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "    \n",
    "    return euro_train, euro_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "EN : We have all become only too used to food scares concerning salmonella in eggs, risk from BSE and e-coli in beef and pesticides residues in carrots.\n",
      "FR : Nous ne connaissons que trop bien les inquiétudes alimentaires liées à la salmonelle dans les ufs, le risque de l'ESB et l'Ecoli dans la viande de buf et les résidus de pesticides dans les carottes.\n",
      "\n",
      "--- val example 1 ---\n",
      "EN : We assert our right to develop and continue developing our own food culture, and it does not include hormones.\n",
      "FR : Nous prétendons avoir le droit de continuer à développer notre propre culture de l'alimentation et de la consommation, dont les hormones ne font pas partie.\n",
      "\n",
      "--- val example 2 ---\n",
      "EN : Ease of access means that dissemination should be fairly easy, but Parliament considers dissemination to be an important stage in this process.\n",
      "FR : Pour faciliter l'accès au programme, il faut rendre sa diffusion relativement facile, mais le Parlement est d'avis que la diffusion est un moment important du processus.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données brutes\n",
    "train_ds, val_ds = load_data()\n",
    "\n",
    "def show_raw_example(ds, idx=0, prefix=\"train\"):\n",
    "    ex = ds[idx]\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"EN :\", ex[\"translation\"][\"en\"])\n",
    "    print(\"FR :\", ex[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "# 3 exemples du train et 3 de la val\n",
    "for i in range(3):\n",
    "    show_raw_example(train_ds, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_raw_example(val_ds, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # Encodage des entrées\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encodage des cibles\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # ignorer les PAD dans la loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ffe0be-9733-4bcd-b503-e03520f3c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                            | 0/80000 [00:00<?, ? examples/s]C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████████████████| 80000/80000 [00:10<00:00, 7818.47 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 7417.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Charger modèle & tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "# Re-tokeniser\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f5ed41-0751-4860-aabb-b62d122c6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_example(raw_ds, tokenized_ds, idx=0, prefix=\"train\"):\n",
    "    raw = raw_ds[idx]\n",
    "    tok = tokenized_ds[idx]\n",
    "\n",
    "    print(f\"=== {prefix} example {idx} ===\")\n",
    "    print(\"RAW EN :\", raw[\"translation\"][\"en\"])\n",
    "    print(\"RAW FR :\", raw[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "    # Inputs\n",
    "    print(\"input_ids[:20] :\", tok[\"input_ids\"][:20])\n",
    "    print(\"Decoded input  :\", tokenizer.decode(tok[\"input_ids\"], skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "    # Labels (enlevant les -100 pour re-décoder)\n",
    "    labels = tok[\"labels\"]\n",
    "    # On remplace les -100 par pad_token_id pour pouvoir décoder\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels_for_decode = [pad_id if x == -100 else x for x in labels]\n",
    "    print(\"labels[:20]     :\", labels[:20])\n",
    "    print(\"Decoded labels  :\", tokenizer.decode(labels_for_decode, skip_special_tokens=True))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1ad48a-590c-4800-b266-50f560637202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train example 0 ===\n",
      "RAW EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "RAW FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 282, 1611, 7, 6, 28, 69, 351, 6, 69, 1543, 13, 3065, 11, 69]\n",
      "Decoded input  : Translate English to French: As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "\n",
      "labels[:20]     : [622, 2430, 3890, 35, 7, 238, 678, 7056, 6, 393, 1089, 11183, 6, 1089, 1543, 20, 50, 25060, 3, 15]\n",
      "Decoded labels  : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "=== train example 1 ===\n",
      "RAW EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "RAW FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 94, 405, 5071, 2516, 21, 1402, 24, 273, 113, 33, 300, 8, 3, 22469]\n",
      "Decoded input  : Translate English to French: It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "\n",
      "labels[:20]     : [802, 259, 3, 26, 31, 6752, 29500, 238, 110, 3787, 285, 21442, 29, 17, 3, 8934, 20, 50, 953, 20]\n",
      "Decoded labels  : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "=== train example 2 ===\n",
      "RAW EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "RAW FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 101, 17516, 857, 24, 8, 12326, 19, 1345, 1879, 250, 34, 11708, 7, 3]\n",
      "Decoded input  : Translate English to French: We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "\n",
      "labels[:20]     : [3, 18990, 6, 678, 12182, 106, 7, 238, 90, 29296, 259, 247, 3, 40, 22, 7408, 19185, 7, 288, 6]\n",
      "Decoded labels  : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "=== val example 0 ===\n",
      "RAW EN : We have all become only too used to food scares concerning salmonella in eggs, risk from BSE and e-coli in beef and pesticides residues in carrots.\n",
      "RAW FR : Nous ne connaissons que trop bien les inquiétudes alimentaires liées à la salmonelle dans les ufs, le risque de l'ESB et l'Ecoli dans la viande de buf et les résidus de pesticides dans les carottes.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 101, 43, 66, 582, 163, 396, 261, 12, 542, 6541, 15, 7, 6238, 15237]\n",
      "Decoded input  : Translate English to French: We have all become only too used to food scares concerning salmonella in eggs, risk from BSE and e-coli in beef and pesticides residues in carrots.\n",
      "\n",
      "labels[:20]     : [2228, 3, 29, 15, 20714, 6577, 238, 5227, 923, 110, 16, 1169, 9643, 7, 20753, 7, 15781, 15, 7, 3]\n",
      "Decoded labels  : Nous ne connaissons que trop bien les inquiétudes alimentaires liées à la salmonelle dans les ufs, le risque de l'ESB et l'Ecoli dans la viande de buf et les résidus de pesticides dans les carottes.\n",
      "\n",
      "=== val example 1 ===\n",
      "RAW EN : We assert our right to develop and continue developing our own food culture, and it does not include hormones.\n",
      "RAW FR : Nous prétendons avoir le droit de continuer à développer notre propre culture de l'alimentation et de la consommation, dont les hormones ne font pas partie.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 101, 15376, 69, 269, 12, 1344, 11, 916, 2421, 69, 293, 542, 1543, 6]\n",
      "Decoded input  : Translate English to French: We assert our right to develop and continue developing our own food culture, and it does not include hormones.\n",
      "\n",
      "labels[:20]     : [2228, 2811, 31667, 3517, 90, 6388, 20, 916, 52, 3, 85, 16710, 1680, 6999, 1543, 20, 3, 40, 31, 22193]\n",
      "Decoded labels  : Nous prétendons avoir le droit de continuer à développer notre propre culture de l'alimentation et de la consommation, dont les hormones ne font pas partie.\n",
      "\n",
      "=== val example 2 ===\n",
      "RAW EN : Ease of access means that dissemination should be fairly easy, but Parliament considers dissemination to be an important stage in this process.\n",
      "RAW FR : Pour faciliter l'accès au programme, il faut rendre sa diffusion relativement facile, mais le Parlement est d'avis que la diffusion est un moment important du processus.\n",
      "\n",
      "input_ids[:20] : [30355, 15, 1566, 12, 2379, 10, 262, 9, 7, 15, 13, 592, 598, 24, 31363, 225, 36, 5163, 514, 6]\n",
      "Decoded input  : Translate English to French: Ease of access means that dissemination should be fairly easy, but Parliament considers dissemination to be an important stage in this process.\n",
      "\n",
      "labels[:20]     : [1474, 8847, 49, 3, 40, 31, 8080, 185, 2486, 6, 3, 173, 3189, 3, 7723, 3, 7, 9, 21740, 5237]\n",
      "Decoded labels  : Pour faciliter l'accès au programme, il faut rendre sa diffusion relativement facile, mais le Parlement est d'avis que la diffusion est un moment important du processus.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_processed_example(train_ds, tokenized_train, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_processed_example(val_ds, tokenized_val, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    # eval_preds peut être un tuple (preds, labels)\n",
    "    # ou un objet EvalPrediction avec .predictions et .label_ids\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement le 1er élément\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # On met tout en np.array pour être tranquille\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    # Cas où preds = logits (batch, seq_len, vocab_size) -> on prend l'argmax\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # On s'assure que ce sont bien des entiers\n",
    "    preds = preds.astype(\"int64\")\n",
    "\n",
    "    # si jamais il y a des valeurs négatives dans preds on les remplace par pad_token_id avant decode\n",
    "    preds[preds < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Gestion des labels : on remet pad_token_id à la place des -100 pour décoder\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode des prédictions et des labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33188-476e-43b6-81a8-34394241864b",
   "metadata": {},
   "source": [
    "## Métriques sur modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b37d9e-9dea-4adc-8bbf-5e6ce0783d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 7169.00 examples/s]\n",
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_27124\\336556330.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4469153881072998, 'eval_model_preparation_time': 0.003, 'eval_bleu': 24.503120896483793, 'eval_meteor': 0.49564799912880275, 'eval_rouge1': 0.5573144763373334, 'eval_rouge2': 0.33833429470799736, 'eval_rougeL': 0.5113933142445597, 'eval_runtime': 170.6516, 'eval_samples_per_second': 4.688, 'eval_steps_per_second': 0.586}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "BASE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train_base, tokenized_val_base = tokenize_datasets(train_ds, val_ds, tokenizer_base)\n",
    "\n",
    "\n",
    "data_collator_base = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_base,\n",
    "    model=model_base,\n",
    ")\n",
    "\n",
    "eval_args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"baseline_flan_t5_en_fr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    # paramètres de génération cohérents avec ceux du fine-tuning\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"logs_baseline\",\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_base,\n",
    "    args=eval_args_base,\n",
    "    eval_dataset=tokenized_val_base,\n",
    "    tokenizer=tokenizer_base,\n",
    "    data_collator=data_collator_base,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer_base),\n",
    ")\n",
    "\n",
    "baseline_metrics = trainer_base.evaluate()\n",
    "print(baseline_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_flan_t5_en_fr\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu\",  \n",
    "        greater_is_better=True,       \n",
    "        logging_steps=50,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()  \n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarde du modèle LoRA + tokenizer dans le même dossier\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60fee12-2491-42a8-aecc-beb8a9ac746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "\n",
    "    input_text = INSTRUCTION + sentence\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)  # les tensors vont sur le même device que le modèle\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca642d21-53be-46d4-a157-e3e6337e00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_27124\\1523741927.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22236' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22236/50000 58:11 < 1:12:39, 6.37 it/s, Epoch 2.22/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.783200</td>\n",
       "      <td>1.445070</td>\n",
       "      <td>6.318779</td>\n",
       "      <td>0.269898</td>\n",
       "      <td>0.382660</td>\n",
       "      <td>0.216753</td>\n",
       "      <td>0.348862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.650900</td>\n",
       "      <td>1.437017</td>\n",
       "      <td>6.397862</td>\n",
       "      <td>0.270863</td>\n",
       "      <td>0.382085</td>\n",
       "      <td>0.217926</td>\n",
       "      <td>0.349424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m train_ds, val_ds = load_data()\n\u001b[32m      4\u001b[39m tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m trainer = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, tokenizer, tokenized_train, tokenized_val)\u001b[39m\n\u001b[32m     20\u001b[39m gc_callback = GarbageCollectorCallback()  \n\u001b[32m     22\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     23\u001b[39m     model=model,\n\u001b[32m     24\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     callbacks=[gc_callback],\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Sauvegarde du modèle LoRA + tokenizer dans le même dossier\u001b[39;00m\n\u001b[32m     35\u001b[39m trainer.model.save_pretrained(training_args.output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    train_ds, val_ds = load_data()\n",
    "    tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "    trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca0fd8-ae8d-479f-bde0-acce1fb23652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "PEFT_DIR = \"./finetuned_flan_t5_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Lire la config PEFT (LoRA) depuis le dossier local\n",
    "peft_config = PeftConfig.from_pretrained(PEFT_DIR, local_files_only=True)\n",
    "\n",
    "# 2) Charger le modèle de base utilisé pendant le fine-tuning\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "\n",
    "# 3) Appliquer les poids LoRA entraînés\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 4) Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "print(\"Modèle LoRA + tokenizer rechargés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85a61a-431c-43ae-84b6-0f42ebda2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Quelques phrases du dataset train\n",
    "for i in range(3):\n",
    "    en = train_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = train_ds[i][\"translation\"][\"fr\"]\n",
    "    fr_pred = translate_sentence(en, model, tokenizer)\n",
    "    print(\"=== Exemple train\", i, \"===\")\n",
    "    print(\"EN     :\", en)\n",
    "    print(\"FR gold:\", fr_gold)\n",
    "    print(\"FR pred:\", fr_pred)\n",
    "    print()\n",
    "\n",
    "# 2) phrases de test perso\n",
    "tests = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"EN :\", s)\n",
    "    print(\"FR :\", translate_sentence(s, model, tokenizer))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b53398-1fdb-49b0-907c-fc4c86aac63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c99d7d-f98d-4911-bfc7-ed5755160283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate_sentence(\"This is a small translation test.\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381ba35-2591-4787-88c1-f773526a1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

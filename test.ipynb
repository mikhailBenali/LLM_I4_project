{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628a866-c0b7-426c-b766-c12d8e1e3bdd",
   "metadata": {},
   "source": [
    "## Ce qu’il ne faut pas faire pour le modèle T5\n",
    "\n",
    "- Pas de lemmatisation / stemming : tu casserais la forme exacte que le modèle attend.\n",
    "\n",
    "- Pas de suppression de stopwords (“le, de, et…”) : les Transformers utilisent ces mots pour comprendre la syntaxe.\n",
    "\n",
    "- Pas de passage forcé en minuscules si le modèle a été pré-entraîné en respectant la casse.\n",
    "\n",
    "- Pas de suppression massive de ponctuation (les modèles utilisent “?”, “.”, “,”, “:” pour le sens et la segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "**OPUS et Europarl**\n",
    "\n",
    "**OPUS-100 en-fr** : mélange de sources, phrases souvent plus courtes, variées (titres, sous-phrases, etc.).\n",
    "\n",
    "**Europarl en-fr** : phrases plus longues, style plus formel, discours parlementaires → plus de contexte par phrase.\n",
    "\n",
    "Du point de vue du modèle :\n",
    "\n",
    "- OPUS -> donne de la diversité (beaucoup de styles, domaines).\n",
    "\n",
    "- Europarl ->  apprend à gérer des phrases longues / syntaxe compliquée (subordonnées, tournures formelles) => **augmenter** ***max_length*** pour moins tronquer les phrases\n",
    "\n",
    "\n",
    "**Ajouter les TED Talks plus tard pour le langage oral, plus “conversational”, avec un ton explicatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a070bbb-6532-4155-a50f-f2a75d14be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Paramètres de décodage par défaut pour .generate()\n",
    "    # pendant l’éval (puisque predict_with_generate=True), les traductions seront générées avec ces paramètres -> métriques qui reflètent ce décodage\n",
    "    # pendant l’inférence manuelle (model.generate(...) sans redonner ces arguments), ce seront les valeurs par défaut.\n",
    "    \n",
    "    model.config.num_beams = 4              # beam search\n",
    "    model.config.length_penalty = 0.9       # < 1 = un peu plus court, > 1 = plus long\n",
    "    model.config.no_repeat_ngram_size = 3   # évite de répéter des 3-grammes\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data(sample_size_train=10000, sample_size_val=300):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=43)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val   = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds   = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement les ids\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode des prédictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # On remet pad_token_id à la place des -100 pour décoder correctement les labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_flan_t5_en_fr\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=30,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu\",  # dire que le “best” = BLEU\n",
    "        greater_is_better=True,             # plus le BLEU est grand, mieux c’est\n",
    "        logging_steps=50,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()  \n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarde du modèle LoRA + tokenizer dans le même dossier\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best BLEU :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60fee12-2491-42a8-aecc-beb8a9ac746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "\n",
    "    input_text = INSTRUCTION + sentence\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)  # les tensors vont sur le même device que le modèle\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca642d21-53be-46d4-a157-e3e6337e00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                           | 0/160000 [00:00<?, ? examples/s]C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████████████████████████████████████████████████████| 160000/160000 [00:18<00:00, 8689.57 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 7595.01 examples/s]\n",
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_25460\\2713340983.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254435' max='300000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254435/300000 10:58:43 < 1:57:57, 6.44 it/s, Epoch 12.72/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.278570</td>\n",
       "      <td>5.887960</td>\n",
       "      <td>0.264857</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.183931</td>\n",
       "      <td>0.335888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.274634</td>\n",
       "      <td>5.988416</td>\n",
       "      <td>0.267820</td>\n",
       "      <td>0.368303</td>\n",
       "      <td>0.188915</td>\n",
       "      <td>0.340476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.272323</td>\n",
       "      <td>6.126513</td>\n",
       "      <td>0.269592</td>\n",
       "      <td>0.371732</td>\n",
       "      <td>0.192173</td>\n",
       "      <td>0.343471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>6.201977</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.377212</td>\n",
       "      <td>0.195875</td>\n",
       "      <td>0.348648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.269259</td>\n",
       "      <td>6.289114</td>\n",
       "      <td>0.274420</td>\n",
       "      <td>0.373912</td>\n",
       "      <td>0.194756</td>\n",
       "      <td>0.346188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.267699</td>\n",
       "      <td>6.290378</td>\n",
       "      <td>0.272604</td>\n",
       "      <td>0.371851</td>\n",
       "      <td>0.195662</td>\n",
       "      <td>0.345254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.266879</td>\n",
       "      <td>6.306374</td>\n",
       "      <td>0.274801</td>\n",
       "      <td>0.373236</td>\n",
       "      <td>0.195021</td>\n",
       "      <td>0.346074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.265778</td>\n",
       "      <td>6.288759</td>\n",
       "      <td>0.273940</td>\n",
       "      <td>0.373726</td>\n",
       "      <td>0.194846</td>\n",
       "      <td>0.347219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.265371</td>\n",
       "      <td>6.459604</td>\n",
       "      <td>0.275065</td>\n",
       "      <td>0.375743</td>\n",
       "      <td>0.201117</td>\n",
       "      <td>0.349795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.264779</td>\n",
       "      <td>6.446556</td>\n",
       "      <td>0.274149</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>0.348987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.263994</td>\n",
       "      <td>6.397631</td>\n",
       "      <td>0.277075</td>\n",
       "      <td>0.375182</td>\n",
       "      <td>0.200291</td>\n",
       "      <td>0.349789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.264311</td>\n",
       "      <td>6.496296</td>\n",
       "      <td>0.277488</td>\n",
       "      <td>0.378306</td>\n",
       "      <td>0.202249</td>\n",
       "      <td>0.352626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    train_ds, val_ds = load_data()\n",
    "    tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "    trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6ca0fd8-ae8d-479f-bde0-acce1fb23652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss                                  NaN\n",
      "grad_norm                             NaN\n",
      "learning_rate                         NaN\n",
      "epoch                           15.000000\n",
      "step                        300000.000000\n",
      "eval_loss                        0.263705\n",
      "eval_bleu                        6.582103\n",
      "eval_meteor                      0.279878\n",
      "eval_rouge1                      0.378510\n",
      "eval_rouge2                      0.203669\n",
      "eval_rougeL                      0.352321\n",
      "eval_runtime                    31.712500\n",
      "eval_samples_per_second         18.920000\n",
      "eval_steps_per_second            2.365000\n",
      "train_runtime                         NaN\n",
      "train_samples_per_second              NaN\n",
      "train_steps_per_second                NaN\n",
      "total_flos                            NaN\n",
      "train_loss                            NaN\n",
      "Name: 6014, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'évaluation (celles qui contiennent eval_loss / eval_bleu, etc.)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Dernière évaluation\n",
    "last_eval = eval_logs.iloc[-1]\n",
    "print(last_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "PEFT_DIR = \"./finetuned_flan_t5_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle LoRA + tokenizer rechargés\n"
     ]
    }
   ],
   "source": [
    "# 1) Lire la config PEFT (LoRA) depuis le dossier local\n",
    "peft_config = PeftConfig.from_pretrained(PEFT_DIR, local_files_only=True)\n",
    "\n",
    "# 2) Charger le modèle de base utilisé pendant le fine-tuning\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "\n",
    "# 3) Appliquer les poids LoRA entraînés\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 4) Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "# ou, si tu avais fait tokenizer.save_pretrained(PEFT_DIR) :\n",
    "# tokenizer = AutoTokenizer.from_pretrained(PEFT_DIR)\n",
    "\n",
    "print(\"Modèle LoRA + tokenizer rechargés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EN : Hello, how are you?\n",
      "FR : Bonjour, comment tu ?\n",
      "\n",
      "EN : This project is about automatic translation.\n",
      "FR : Ce projet concerne la traduction automatique.\n",
      "\n",
      "EN : The weather is nice today.\n",
      "FR : Le temps est agréable aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in test_sentences:\n",
    "    print(\"\\nEN :\", s)\n",
    "    print(\"FR :\", translate_sentence(s, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c99d7d-f98d-4911-bfc7-ed5755160283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un petit test de traduction.\n"
     ]
    }
   ],
   "source": [
    "print(translate_sentence(\"This is a small translation test.\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381ba35-2591-4787-88c1-f773526a1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

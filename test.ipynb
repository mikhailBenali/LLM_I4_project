{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628a866-c0b7-426c-b766-c12d8e1e3bdd",
   "metadata": {},
   "source": [
    "## Ce qu’il ne faut pas faire pour le modèle T5\n",
    "\n",
    "- Pas de lemmatisation / stemming : tu casserais la forme exacte que le modèle attend.\n",
    "\n",
    "- Pas de suppression de stopwords (“le, de, et…”) : les Transformers utilisent ces mots pour comprendre la syntaxe.\n",
    "\n",
    "- Pas de passage forcé en minuscules si le modèle a été pré-entraîné en respectant la casse.\n",
    "\n",
    "- Pas de suppression massive de ponctuation (les modèles utilisent “?”, “.”, “,”, “:” pour le sens et la segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "**OPUS et Europarl**\n",
    "\n",
    "**OPUS-100 en-fr** : mélange de sources, phrases souvent plus courtes, variées (titres, sous-phrases, etc.).\n",
    "\n",
    "**Europarl en-fr** : phrases plus longues, style plus formel, discours parlementaires → plus de contexte par phrase.\n",
    "\n",
    "Du point de vue du modèle :\n",
    "\n",
    "- OPUS -> donne de la diversité (beaucoup de styles, domaines).\n",
    "\n",
    "- Europarl ->  apprend à gérer des phrases longues / syntaxe compliquée (subordonnées, tournures formelles) => **augmenter** ***max_length*** pour moins tronquer les phrases\n",
    "\n",
    "\n",
    "**Ajouter les TED Talks plus tard pour le langage oral, plus “conversational”, avec un ton explicatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a070bbb-6532-4155-a50f-f2a75d14be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Paramètres de décodage par défaut pour .generate()\n",
    "    # pendant l’éval (puisque predict_with_generate=True), les traductions seront générées avec ces paramètres -> métriques qui reflètent ce décodage\n",
    "    # pendant l’inférence manuelle (model.generate(...) sans redonner ces arguments), ce seront les valeurs par défaut.\n",
    "    \n",
    "    model.config.num_beams = 4              # beam search\n",
    "    model.config.length_penalty = 0.9       # < 1 = un peu plus court, > 1 = plus long\n",
    "    model.config.no_repeat_ngram_size = 3   # évite de répéter des 3-grammes\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data(sample_size_train=10000, sample_size_val=300):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=43)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val   = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds   = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement les ids\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode des prédictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # On remet pad_token_id à la place des -100 pour décoder correctement les labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "\n",
    "        \n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_flan_t5_en_fr\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=30,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",  \n",
    "        greater_is_better=False,       \n",
    "        logging_steps=50,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()  \n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarde du modèle LoRA + tokenizer dans le même dossier\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60fee12-2491-42a8-aecc-beb8a9ac746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "\n",
    "    input_text = INSTRUCTION + sentence\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)  # les tensors vont sur le même device que le modèle\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca642d21-53be-46d4-a157-e3e6337e00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_6216\\3565460997.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75000' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75000/75000 3:31:09, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.277945</td>\n",
       "      <td>6.258771</td>\n",
       "      <td>0.261330</td>\n",
       "      <td>0.359372</td>\n",
       "      <td>0.177789</td>\n",
       "      <td>0.332368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.272326</td>\n",
       "      <td>6.509489</td>\n",
       "      <td>0.263228</td>\n",
       "      <td>0.361340</td>\n",
       "      <td>0.180543</td>\n",
       "      <td>0.332797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.269727</td>\n",
       "      <td>6.417688</td>\n",
       "      <td>0.262687</td>\n",
       "      <td>0.363001</td>\n",
       "      <td>0.178816</td>\n",
       "      <td>0.333536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.267651</td>\n",
       "      <td>6.433599</td>\n",
       "      <td>0.264593</td>\n",
       "      <td>0.363964</td>\n",
       "      <td>0.180733</td>\n",
       "      <td>0.335548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.267071</td>\n",
       "      <td>6.669093</td>\n",
       "      <td>0.266846</td>\n",
       "      <td>0.366098</td>\n",
       "      <td>0.182794</td>\n",
       "      <td>0.338517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.265574</td>\n",
       "      <td>6.647753</td>\n",
       "      <td>0.265751</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.185565</td>\n",
       "      <td>0.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.264946</td>\n",
       "      <td>6.514973</td>\n",
       "      <td>0.264489</td>\n",
       "      <td>0.365101</td>\n",
       "      <td>0.183196</td>\n",
       "      <td>0.335902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.264099</td>\n",
       "      <td>6.601684</td>\n",
       "      <td>0.269214</td>\n",
       "      <td>0.369466</td>\n",
       "      <td>0.186579</td>\n",
       "      <td>0.341583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.263701</td>\n",
       "      <td>6.693993</td>\n",
       "      <td>0.273334</td>\n",
       "      <td>0.373951</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>0.345634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.263391</td>\n",
       "      <td>6.713036</td>\n",
       "      <td>0.271984</td>\n",
       "      <td>0.373470</td>\n",
       "      <td>0.188961</td>\n",
       "      <td>0.344164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.263937</td>\n",
       "      <td>6.572273</td>\n",
       "      <td>0.272996</td>\n",
       "      <td>0.374174</td>\n",
       "      <td>0.187367</td>\n",
       "      <td>0.344244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.262175</td>\n",
       "      <td>6.500738</td>\n",
       "      <td>0.270215</td>\n",
       "      <td>0.373338</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.343555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.262407</td>\n",
       "      <td>6.729949</td>\n",
       "      <td>0.271882</td>\n",
       "      <td>0.374655</td>\n",
       "      <td>0.190465</td>\n",
       "      <td>0.345584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.261609</td>\n",
       "      <td>6.670890</td>\n",
       "      <td>0.273576</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.191966</td>\n",
       "      <td>0.345971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.262188</td>\n",
       "      <td>6.720405</td>\n",
       "      <td>0.273035</td>\n",
       "      <td>0.373598</td>\n",
       "      <td>0.191654</td>\n",
       "      <td>0.344685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.262150</td>\n",
       "      <td>6.677358</td>\n",
       "      <td>0.270855</td>\n",
       "      <td>0.370421</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>0.342791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.261874</td>\n",
       "      <td>6.746830</td>\n",
       "      <td>0.274889</td>\n",
       "      <td>0.374570</td>\n",
       "      <td>0.192412</td>\n",
       "      <td>0.345769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>6.687761</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>0.374787</td>\n",
       "      <td>0.192960</td>\n",
       "      <td>0.345293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.261230</td>\n",
       "      <td>6.847085</td>\n",
       "      <td>0.279591</td>\n",
       "      <td>0.378408</td>\n",
       "      <td>0.195623</td>\n",
       "      <td>0.349352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.261564</td>\n",
       "      <td>6.832802</td>\n",
       "      <td>0.277101</td>\n",
       "      <td>0.376119</td>\n",
       "      <td>0.196044</td>\n",
       "      <td>0.347787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.260992</td>\n",
       "      <td>6.671963</td>\n",
       "      <td>0.277915</td>\n",
       "      <td>0.375782</td>\n",
       "      <td>0.194166</td>\n",
       "      <td>0.346773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.261155</td>\n",
       "      <td>6.881605</td>\n",
       "      <td>0.277889</td>\n",
       "      <td>0.377914</td>\n",
       "      <td>0.196123</td>\n",
       "      <td>0.349040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.260553</td>\n",
       "      <td>6.794091</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>0.375796</td>\n",
       "      <td>0.195293</td>\n",
       "      <td>0.347319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.260731</td>\n",
       "      <td>6.872491</td>\n",
       "      <td>0.278229</td>\n",
       "      <td>0.377168</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.348349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.260631</td>\n",
       "      <td>6.912389</td>\n",
       "      <td>0.278789</td>\n",
       "      <td>0.378441</td>\n",
       "      <td>0.196934</td>\n",
       "      <td>0.349462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.260586</td>\n",
       "      <td>6.962144</td>\n",
       "      <td>0.280077</td>\n",
       "      <td>0.381073</td>\n",
       "      <td>0.198424</td>\n",
       "      <td>0.351892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.260418</td>\n",
       "      <td>6.916975</td>\n",
       "      <td>0.279652</td>\n",
       "      <td>0.380604</td>\n",
       "      <td>0.199542</td>\n",
       "      <td>0.351007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.260594</td>\n",
       "      <td>6.919853</td>\n",
       "      <td>0.280125</td>\n",
       "      <td>0.381246</td>\n",
       "      <td>0.198954</td>\n",
       "      <td>0.351782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.260527</td>\n",
       "      <td>6.904703</td>\n",
       "      <td>0.279946</td>\n",
       "      <td>0.380240</td>\n",
       "      <td>0.198410</td>\n",
       "      <td>0.351405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.260510</td>\n",
       "      <td>6.910789</td>\n",
       "      <td>0.279674</td>\n",
       "      <td>0.380840</td>\n",
       "      <td>0.198769</td>\n",
       "      <td>0.351781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BLEU : 0.2604176104068756\n",
      "Best checkpoint : finetuned_flan_t5_en_fr\\checkpoint-67500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    train_ds, val_ds = load_data()\n",
    "    tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "    trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ca0fd8-ae8d-479f-bde0-acce1fb23652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch   step  eval_loss  eval_bleu  eval_meteor  eval_rougeL\n",
      "50      1.0   2500   0.277945   6.258771     0.261330     0.332368\n",
      "101     2.0   5000   0.272326   6.509489     0.263228     0.332797\n",
      "152     3.0   7500   0.269727   6.417688     0.262687     0.333536\n",
      "203     4.0  10000   0.267651   6.433599     0.264593     0.335548\n",
      "254     5.0  12500   0.267071   6.669093     0.266846     0.338517\n",
      "305     6.0  15000   0.265574   6.647753     0.265751     0.337100\n",
      "356     7.0  17500   0.264946   6.514973     0.264489     0.335902\n",
      "407     8.0  20000   0.264099   6.601684     0.269214     0.341583\n",
      "458     9.0  22500   0.263701   6.693993     0.273334     0.345634\n",
      "509    10.0  25000   0.263391   6.713036     0.271984     0.344164\n",
      "560    11.0  27500   0.263937   6.572273     0.272996     0.344244\n",
      "611    12.0  30000   0.262175   6.500738     0.270215     0.343555\n",
      "662    13.0  32500   0.262407   6.729949     0.271882     0.345584\n",
      "713    14.0  35000   0.261609   6.670890     0.273576     0.345971\n",
      "764    15.0  37500   0.262188   6.720405     0.273035     0.344685\n",
      "815    16.0  40000   0.262150   6.677358     0.270855     0.342791\n",
      "866    17.0  42500   0.261874   6.746830     0.274889     0.345769\n",
      "917    18.0  45000   0.262010   6.687761     0.275294     0.345293\n",
      "968    19.0  47500   0.261230   6.847085     0.279591     0.349352\n",
      "1019   20.0  50000   0.261564   6.832802     0.277101     0.347787\n",
      "1070   21.0  52500   0.260992   6.671963     0.277915     0.346773\n",
      "1121   22.0  55000   0.261155   6.881605     0.277889     0.349040\n",
      "1172   23.0  57500   0.260553   6.794091     0.275628     0.347319\n",
      "1223   24.0  60000   0.260731   6.872491     0.278229     0.348349\n",
      "1274   25.0  62500   0.260631   6.912389     0.278789     0.349462\n",
      "1325   26.0  65000   0.260586   6.962144     0.280077     0.351892\n",
      "1376   27.0  67500   0.260418   6.916975     0.279652     0.351007\n",
      "1427   28.0  70000   0.260594   6.919853     0.280125     0.351782\n",
      "1478   29.0  72500   0.260527   6.904703     0.279946     0.351405\n",
      "1529   30.0  75000   0.260510   6.910789     0.279674     0.351781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "PEFT_DIR = \"./finetuned_flan_t5_en_fr/checkpoint-\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle LoRA + tokenizer rechargés\n"
     ]
    }
   ],
   "source": [
    "# 1) Lire la config PEFT (LoRA) depuis le dossier local\n",
    "peft_config = PeftConfig.from_pretrained(PEFT_DIR, local_files_only=True)\n",
    "\n",
    "# 2) Charger le modèle de base utilisé pendant le fine-tuning\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "\n",
    "# 3) Appliquer les poids LoRA entraînés\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 4) Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "print(\"Modèle LoRA + tokenizer rechargés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EN : Hello, how are you?\n",
      "FR : Bien, tu ?\n",
      "\n",
      "EN : This project is about automatic translation.\n",
      "FR : Cette projet concerne la traduction automatique.\n",
      "\n",
      "EN : The weather is nice today.\n",
      "FR : Le temps est agréable aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTION = \"Translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in test_sentences:\n",
    "    print(\"\\nEN :\", s)\n",
    "    print(\"FR :\", translate_sentence(s, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67c99d7d-f98d-4911-bfc7-ed5755160283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un petit test de traduction.\n"
     ]
    }
   ],
   "source": [
    "print(translate_sentence(\"This is a small translation test.\", model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381ba35-2591-4787-88c1-f773526a1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091a99c8",
   "metadata": {},
   "source": [
    "# **Fine-tuning d'un modèle de traduction EN→FR avec T5 + LoRA**\n",
    "\n",
    "Dans le cadre du projet de LLM, nous avons développé un système de traduction automatique anglais → français basé sur les modèles modernes de type Transformer. L’objectif principal est d’adapter un modèle pré-entraîné existant afin d’obtenir une solution exploitable dans une tâche de traduction.\n",
    "\n",
    "Nous avons choisi d’utiliser T5 (Text-to-Text Transfer Transformer) comme modèle de base, en raison de son architecture encoder–decoder particulièrement bien adaptée aux tâches de traduction. Pour affiner ses capacités sans réentraîner l’intégralité des 220 millions de paramètres, nous avons adopté la méthode LoRA (Low-Rank Adaptation), qui permet d’ajouter un petit nombre de poids entraînables tout en gelant le modèle original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a9f41-64d6-4976-a9fc-b9e00aa3c7f9",
   "metadata": {},
   "source": [
    "## Membres du Groupe de Projet\n",
    "\n",
    "- Maxence KAMIONKA\n",
    "- Mikhaïl BENALI\n",
    "- Hadja BAH\n",
    "- Emmanuel DAGNOGO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb1608",
   "metadata": {},
   "source": [
    "### **Import des bibliothèques nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cec6d-7770-48d1-9816-0a67035b2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from packaging import version\n",
    "import pprint\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict, load_from_disk\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftConfig, PeftModel\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "try:\n",
    "    import langid\n",
    "    langid.set_languages([\"en\", \"fr\"])\n",
    "    HAS_LANGID = True\n",
    "except Exception:\n",
    "    HAS_LANGID = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd78f78-71ef-42b0-ac1c-869ca9b16332",
   "metadata": {},
   "source": [
    "Ici nous utilisons PyTorch afin de détécter si un GPU est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1790e60-435d-47f2-80f1-a2e371c9f961",
   "metadata": {},
   "source": [
    "Nous utilisons également gc, un garbage collector, permettant de vider la mémoire RAM d'éléments inutiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f9ae-8932-4f97-b23c-ce6df94c4421",
   "metadata": {},
   "source": [
    "## **Test du modèle de base**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e2170-2f8b-4f9c-8053-5e8d0d30a311",
   "metadata": {},
   "source": [
    "Nous sommes partis du modèle T5-base, disponible via la bibliothèque Transformers de HuggingFace, ainsi que du tokenizer associé. Pour orienter le modèle vers la tâche de traduction, nous avons utilisé une instruction : `translate English to French`.\n",
    "\n",
    "Cette instruction joue un rôle essentiel : dans l’architecture T5, toutes les tâches sont formulées comme une entrée textuelle. Ainsi, le modèle ne devine pas la tâche à accomplir ; il s’appuie entièrement sur le préfixe pour déterminer l’opération attendue. En ajoutant systématiquement ce préfixe au texte source, nous indiquons clairement au modèle qu’il doit réaliser une traduction de l’anglais vers le français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e092c15-9e28-428f-85b2-c5e1c5ab8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "INSTRUCTION = \"translate English to French: \"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e6434-1fcb-4810-9a7c-46782fb9850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7ed64-717b-4ff0-b732-dda9fd992d4c",
   "metadata": {},
   "source": [
    "## **Utilisation de LoRa (Low-Rank Adaption)**\n",
    "\n",
    "On utilise LoRA afin d'entrainer des paramètres extra tout en gelant ceux du modèle de base pour ne pas lui faire oublier ce qu'il sait déjà faire originellement. On tune alors ces poids supplémentaires plutôt que tout le modèle.\n",
    "\n",
    "Nous utilisons LoRA (Low-Rank Adaptation) afin d’entraîner uniquement un petit sous-ensemble de paramètres additionnels, tout en gelant les poids du modèle de base. Cette stratégie évite d’oublier les compétences déjà acquises par T5 lors du pré-entraînement, réduit fortement le coût mémoire/temps. \n",
    "\n",
    "Autrement dit, au lieu d’ajuster tous les poids de T5, nous n’entraînons que ces poids supplémentaires, ce qui permet une adaptation efficace et stable à la tâche de traduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372ab3b-4cec-44a2-8c4f-d9ea73344231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_model():\n",
    "    # on réutilise MODEL_NAME & tokenizer définis plus haut\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### **Choix des jeux de données**\n",
    "\n",
    "Après des essais préliminaires, le dataset OPUS a été mis de côté : nous avons constaté des traductions un peu \"étranges\", susceptibles d’introduire du bruit dans le modèle.\n",
    "\n",
    "Nous avons donc privilégié Europarl.\n",
    "\n",
    "**Europarl en-fr** : phrases longues, style plus formel, discours parlementaires -> plus de contexte par phrase.\n",
    "\n",
    "**On a préféré augmenter le nombre de données (ici 700K avec 0.5% soit 3500 paires en validation) plutôt qu'augmenter le nombre d'epochs (ici 2) pour éviter le sur-apprentissage et améliorer la généralisation du modèle en lui donnant davantage de phrases, tournures et vocabulaire.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### **Charger et échantilloner le dataset OPUS et Europarl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_old(sample_size_train=80000, sample_size_val=1000):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def load_data(sample_size_train=700000, sample_size_val=3500):\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "    \n",
    "    return euro_train, euro_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495c898",
   "metadata": {},
   "source": [
    "### **Chargement des données brutes et affichage de quelques exemples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "EN : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "FR : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "--- val example 1 ---\n",
      "EN : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "FR : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- val example 2 ---\n",
      "EN : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "FR : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données brutes\n",
    "train_ds, val_ds = load_data()\n",
    "\n",
    "def show_raw_example(ds, idx=0, prefix=\"train\"):\n",
    "    ex = ds[idx]\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"EN :\", ex[\"translation\"][\"en\"])\n",
    "    print(\"FR :\", ex[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "# 3 exemples du train et 3 de la val\n",
    "for i in range(3):\n",
    "    show_raw_example(train_ds, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_raw_example(val_ds, i, prefix=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### **Encoder l’anglais comme input et le français comme labels**\n",
    "\n",
    "Cette fonction prépare les données en encodant l’anglais comme entrée du modèle et le français comme cible d’apprentissage, tout en masquant les tokens de padding pour que ceux-ci n’influencent pas la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # Encodage des entrées\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encodage des cibles\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # ignorer les PAD dans la loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea8210",
   "metadata": {},
   "source": [
    "### **Chargement du modèle de base et du tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffe0be-9733-4bcd-b503-e03520f3c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 223,788,288 || trainable%: 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                           | 0/700000 [00:00<?, ? examples/s]C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████████████████████████████████████████████████████| 700000/700000 [01:28<00:00, 7885.46 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 3500/3500 [00:00<00:00, 8235.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Charger modèle LoRA & réutiliser le tokenizer global\n",
    "model = load_lora_model()\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51ce2e-1a93-428b-bf34-6e2ffe6a71ff",
   "metadata": {},
   "source": [
    "Voici quelques exemples montrant les embeddings associés aux inputs et labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5ed41-0751-4860-aabb-b62d122c6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_example(raw_ds, tokenized_ds, idx=0, prefix=\"train\"):\n",
    "    raw = raw_ds[idx]\n",
    "    tok = tokenized_ds[idx]\n",
    "\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"RAW EN :\", raw[\"translation\"][\"en\"])\n",
    "    print(\"RAW FR :\", raw[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "    # Inputs\n",
    "    print(\"input_ids[:20] :\", tok[\"input_ids\"][:20])\n",
    "    print(\"Decoded input  :\", tokenizer.decode(tok[\"input_ids\"], skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "    # Labels (enlevant les -100 pour re-décoder)\n",
    "    labels = tok[\"labels\"]\n",
    "    # On remplace les -100 par pad_token_id pour pouvoir décoder\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels_for_decode = [pad_id if x == -100 else x for x in labels]\n",
    "    print(\"labels[:20]     :\", labels[:20])\n",
    "    print(\"Decoded labels  :\", tokenizer.decode(labels_for_decode, skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ad48a-590c-4800-b266-50f560637202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "RAW EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "RAW FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 282, 1611, 7, 6, 28, 69, 351, 6, 69, 1543, 13, 3065, 11, 69, 1456]\n",
      "Decoded input  : translate English to French: As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "\n",
      "labels[:20]     : [622, 2430, 3890, 35, 7, 238, 678, 7056, 6, 393, 1089, 11183, 6, 1089, 1543, 20, 50, 25060, 3, 15]\n",
      "Decoded labels  : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "RAW EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "RAW FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 94, 405, 5071, 2516, 21, 1402, 24, 273, 113, 33, 300, 8, 3, 22469, 953]\n",
      "Decoded input  : translate English to French: It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "\n",
      "labels[:20]     : [802, 259, 3, 26, 31, 6752, 29500, 238, 110, 3787, 285, 21442, 29, 17, 3, 8934, 20, 50, 953, 20]\n",
      "Decoded labels  : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "RAW EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "RAW FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 101, 17516, 857, 24, 8, 12326, 19, 1345, 1879, 250, 34, 11708, 7, 3, 9]\n",
      "Decoded input  : translate English to French: We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "\n",
      "labels[:20]     : [3, 18990, 6, 678, 12182, 106, 7, 238, 90, 29296, 259, 247, 3, 40, 22, 7408, 19185, 7, 288, 6]\n",
      "Decoded labels  : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "RAW EN : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "RAW FR : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 531, 25, 1845, 6, 14595, 6, 24, 125, 62, 33, 5008, 16, 69, 712, 512]\n",
      "Decoded input  : translate English to French: Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "\n",
      "labels[:20]     : [41, 5332, 61, 312, 26864, 7, 2378, 3, 9, 26, 3493, 18, 173, 238, 247, 1680, 6183, 775, 6, 678]\n",
      "Decoded labels  : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "--- val example 1 ---\n",
      "RAW EN : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "RAW FR : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 304, 369, 223, 12, 82, 711, 500, 6, 27, 317, 24, 8, 1880, 35, 729]\n",
      "Decoded input  : translate English to French: To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "\n",
      "labels[:20]     : [695, 10881, 288, 3, 922, 260, 35, 29875, 6, 528, 9790, 238, 90, 3, 287, 1257, 3, 25428, 99, 1880]\n",
      "Decoded labels  : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- val example 2 ---\n",
      "RAW EN : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "RAW FR : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 24624, 6, 3, 9, 7234, 3527, 133, 163, 4368, 552, 8, 414, 13, 8, 215]\n",
      "Decoded input  : translate English to French: Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "\n",
      "labels[:20]     : [7913, 40, 3335, 6, 50, 3527, 813, 3466, 6534, 3, 29, 15, 14770, 9, 238, 4007, 31, 85, 50, 2202]\n",
      "Decoded labels  : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_processed_example(train_ds, tokenized_train, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_processed_example(val_ds, tokenized_val, i, prefix=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### **Métriques d'évaluation de la traduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ccd77-839a-42f7-9dec-0bbb7065af53",
   "metadata": {},
   "source": [
    "Nous avons utilisé 3 métriques afin de mesurer les performances du modèle : \n",
    "\n",
    "La métrique BLEU permet de comparer des n-grammes entre la prédiction et le label afin de mesurer la proximité de la réponse générée avec les différentes réponses attendues.\n",
    "\n",
    "Le ROUGE est principalement utilisé pour évaluer la qualité des résumés automatiques. Il mesure le recall des n-grammes entre le résumé généré et un ou plusieurs résumés de référence. ROUGE-1 évalue les mots individuels, tandis que ROUGE-2 évalue les paires de mots. Un score ROUGE élevé indique que le résumé couvre bien les informations clés du texte original.\n",
    "\n",
    "Le METEOR est une métrique plus flexible que BLEU, car elle prend en compte non seulement la précision des mots, mais aussi les synonymes et leur ordre. Elle utilise des techniques de recall et de précision, ainsi qu’un alignement entre les mots du texte généré et ceux de référence. METEOR est souvent utilisé pour évaluer les traductions et les résumés, parce qu'il offre une évaluation plus nuancée de la qualité linguistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    # eval_preds peut être un tuple (preds, labels)\n",
    "    # ou un objet EvalPrediction avec .predictions et .label_ids\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement le 1er élément\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # On met tout en np.array pour être tranquille\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    # Cas où preds = logits (batch, seq_len, vocab_size) -> on prend l'argmax\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # On s'assure que ce sont bien des entiers\n",
    "    preds = preds.astype(\"int64\")\n",
    "\n",
    "    # si jamais il y a des valeurs négatives dans preds on les remplace par pad_token_id avant decode\n",
    "    preds[preds < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Gestion des labels : on remet pad_token_id à la place des -100 pour décoder\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode des prédictions et des labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33188-476e-43b6-81a8-34394241864b",
   "metadata": {},
   "source": [
    "## **Métriques sur modèle de base**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20f70c-583d-4c23-9b93-0de3ef1ca176",
   "metadata": {},
   "source": [
    "Nous avons d’abord évalué les performances du modèle de base, afin de disposer d’un point de comparaison et de mesurer ensuite l’impact du fine-tuning sur les métriques obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b37d9e-9dea-4adc-8bbf-5e6ce0783d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_38968\\32419789.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Seq2SeqTrainer(\n",
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\generation\\utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='438' max='438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [438/438 12:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics :\n",
      "{'eval_bleu': 37.108577481672434,\n",
      " 'eval_loss': 0.9090045094490051,\n",
      " 'eval_meteor': 0.6088466325807146,\n",
      " 'eval_model_preparation_time': 0.003,\n",
      " 'eval_rouge1': 0.6654090098650332,\n",
      " 'eval_rouge2': 0.4747695313409016,\n",
      " 'eval_rougeL': 0.6229810019071551,\n",
      " 'eval_runtime': 749.3046,\n",
      " 'eval_samples_per_second': 4.671,\n",
      " 'eval_steps_per_second': 0.585}\n"
     ]
    }
   ],
   "source": [
    "# Modèle de base (non fine-tuné)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "model_base.config.no_repeat_ngram_size = 3\n",
    "\n",
    "data_collator_base = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_base,\n",
    ")\n",
    "\n",
    "eval_args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"baseline_t5_base_en_fr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps = 2, \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"logs_baseline\",\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_base,\n",
    "    args=eval_args_base,\n",
    "    eval_dataset=tokenized_val,  # même val que pour LoRA\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_base,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    ")\n",
    "\n",
    "baseline_metrics = trainer_base.evaluate()\n",
    "print(\"Baseline metrics :\")\n",
    "pprint.pprint(baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2a6aa-cf32-4f4b-b32c-19f3e624fbcd",
   "metadata": {},
   "source": [
    "Ici on crée un callback qui va être appelé à la fin de chaque epoch et qui permettra de vider la mémoire inutilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80756e91-3ae7-49f4-9b63-e4d950782b7f",
   "metadata": {},
   "source": [
    "## **Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea8d29-935b-4cd6-aa92-626d13c246b9",
   "metadata": {},
   "source": [
    "On crée ici une fonction d'entrainement.\n",
    "Cette fonction encapsule l’ensemble du processus d’entraînement du modèle. Elle définit les paramètres du fine-tuning (batch size, learning rate, accumulation de gradients, nombre d’époques, stratégies de sauvegarde et d’évaluation), initialise un Seq2SeqTrainer(entraîneur optimisé pour les tâches où une séquence doit être transformée en une autre) et lance l’apprentissage tout en calculant les métriques à chaque époque. \n",
    "\n",
    "À l’issue du processus, le meilleur modèle ainsi que le tokenizer sont sauvegardés pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# per_device_eval_batch_size=8 \n",
    "# gradient_accumulation_steps = 2 réduire la mémoire : stabilité d’un batch effectif de 16, mais avec la mémoire d’un batch de 8 seulement, mais 2 fois plus long\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_t5_base_en_fr\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps = 2, \n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "\n",
    "        generation_max_length=128,\n",
    "        generation_num_beams=4,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36798a0-a59f-4722-b7bb-d59a63a28db4",
   "metadata": {},
   "source": [
    "## **Entrainement du modèle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c788e-f67b-40cf-a380-dd90d9128278",
   "metadata": {},
   "source": [
    "On entraine notre modèle et on l'évalue avec notre jeu de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79af0475-70c3-4909-9e73-9c37d2915e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_38968\\2911972018.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17726' max='87500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17726/87500 1:18:36 < 5:09:26, 3.76 it/s, Epoch 0.41/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Entraîner le modèle LoRA\n",
    "trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n",
    "\n",
    "# 2) Évaluer proprement le modèle LoRA sur le même jeu de validation\n",
    "lora_metrics = trainer.evaluate(eval_dataset=tokenized_val)\n",
    "print(\"Métriques modèle de base :\")\n",
    "pprint.pprint(baseline_metrics)\n",
    "print(\"Métriques modèle LoRA :\")\n",
    "pprint.pprint(lora_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac8f75-50f8-40f5-9183-9dd77b8ca0ee",
   "metadata": {},
   "source": [
    "On montre l'évolution des métriques au fur et à mesure de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbcc92-f4eb-43ec-95fe-050c3fadca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch   step  eval_loss  eval_bleu  eval_meteor  eval_rougeL\n",
      "875     1.0  43750   0.684410  40.336185     0.629312      0.64225\n",
      "1751    2.0  87500   0.683656  40.330786     0.629337      0.64239\n",
      "1753    2.0  87500   0.684410  40.336185     0.629312      0.64225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7c93b",
   "metadata": {},
   "source": [
    "Le modèle T5-base avait obtenu un BLEU de 37.10, un METEOR de 0.608 et un ROUGE-L de 0.622. Après fine-tuning, ces scores montent respectivement à environ 40.33, 0.629 et 0.643. On observe donc une amélioration cohérente sur toutes les métriques, avec un gain  de plus de 3 points BLEU, indiquant des traductions plus fidèles et mieux alignées. La légère hausse des scores METEOR et ROUGE confirme une meilleure couverture lexicale et une structure de phrase plus proche des références."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "## **Recharger le modèle fine-tuné**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BASE_MODEL_NAME = \"t5-base\"\n",
    "PEFT_DIR = \"./finetuned_t5_base_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle de base et LoRA \n",
    "Ici, nous chargeons le modèle de base , le tokenizer ainsi que le modèle LoRA fine-tuné à des fins de tests et de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle de base + modèle LoRA rechargés depuis les chemins.\n"
     ]
    }
   ],
   "source": [
    "# 1) Tokenizer commun (T5-base) : on le charge \"UNE\" seule fois (sauf si vous faites un run de tout) :)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# 2) Modèle de base (non fine-tuné)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME).to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "# 3) Modèle LoRA fine-tuné (chargé depuis le dossier PEFT_DIR)\n",
    "lora_backbone = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    lora_backbone,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ").to(DEVICE)\n",
    "lora_model.eval()\n",
    "\n",
    "print(\"Modèle de base + modèle LoRA rechargés depuis les chemins.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## **Fonction de traduction + tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, tokenizer, max_length=256, num_beams=4):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=3,  # comme dans model_base.config\n",
    "        )\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b162809",
   "metadata": {},
   "source": [
    "### **Tests avec quelques phrases du dataset train**\n",
    "Nous avons pris quelques exemples du dataset d’entraînement et les avons passés dans le modèle de base ainsi que dans le modèle fine-tuné. Cela permet simplement de vérifier que les modèles traduisent correctement les phrases, sans forcément reproduire exactement les références du train. \n",
    "Les deux modèles produisent des traductions proches du sens attendu, et une petite différence notable apparaît : le modèle fine-tuné traduit Irak par Iraq, alors que le modèle de base conserve la forme anglaise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f85a61a-431c-43ae-84b6-0f42ebda2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple train 0 ---\n",
      "EN      : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR gold : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "FR base : En tant qu'Européens, avec notre expérience, notre culture de paix et nos possibilités économiques, nous sommes également appelés à contribuer à un avenir meilleur pour l'Iraq.\n",
      "FR LoRA : En tant qu'Européens, avec notre expérience, notre culture de paix et nos perspectives économiques, nous sommes également appelés à contribuer à un avenir meilleur pour l'Irak.\n",
      "\n",
      "--- Exemple train 1 ---\n",
      "EN      : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR gold : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "FR base : Il est en effet clair que ceux qui se trouvent à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "FR LoRA : Il est en effet clair que ceux qui se trouvent à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "\n",
      "--- Exemple train 2 ---\n",
      "EN      : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR gold : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR base : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR LoRA : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset train\n",
    "for i in range(3):\n",
    "    en = train_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = train_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer) # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple train\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd27510",
   "metadata": {},
   "source": [
    "### **Tests avec quelques phrases du dataset validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf1f59c-975b-4b20-b3ac-db71914d8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple val 0 ---\n",
      "EN      : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "FR gold : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "FR base : Acceptez-vous, Monsieur le Commissaire, que ce que nous faisons dans notre marché unique est une double distorsions, une distorsion entre la bière et le vin, qui est grossièrement injuste, en raison de taux d'imposition ou d’accises nettement différents entre les États membres?\n",
      "FR LoRA : Acceptez-vous, Monsieur le Commissaire, que ce que nous avons à faire dans notre marché unique est une double distorsions, une distorsion entre la bière et le vin, qui est grossièrement injuste, en raison de taux d'imposition ou accises nettement différents entre les États membres?\n",
      "\n",
      "--- Exemple val 1 ---\n",
      "EN      : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "FR gold : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "FR base : Pour revenir à mon point principal, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "FR LoRA : Pour revenir à mon point principal, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- Exemple val 2 ---\n",
      "EN      : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "FR gold : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "FR base : Naturellement, une Commission temporaire ne fonctionnerait qu'à la fin de l'année, lorsque le nouveau Parlement examinera attentivement le choix des commissaires pour le prochain mandat de cinq ans.\n",
      "FR LoRA : Naturellement, une Commission temporaire ne fonctionnerait qu'à la fin de l'année, lorsque le nouveau Parlement examinera attentivement le choix des commissaires pour le prochain mandat de cinq ans.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset validation\n",
    "for i in range(3):\n",
    "    en = val_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = val_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer)  # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple val\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d1dcf",
   "metadata": {},
   "source": [
    "### **Tests avec quelques phrases de test personnelles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b53398-1fdb-49b0-907c-fc4c86aac63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN      : Hello, how are you?\n",
      "FR base : Bonjour, comment êtes-vous?\n",
      "FR LoRA : Bonjour, comment êtes-vous?\n",
      "\n",
      "EN      : This project is about automatic translation.\n",
      "FR base : Ce projet concerne la traduction automatique.\n",
      "FR LoRA : Ce projet concerne la traduction automatique.\n",
      "\n",
      "EN      : The weather is nice today.\n",
      "FR base : Le temps est agréable aujourd'hui.\n",
      "FR LoRA : Le temps est agréable aujourd'hui.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phrases de test perso\n",
    "tests = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"EN      :\", s)\n",
    "    print(\"FR base :\", translate_sentence(s, base_model, tokenizer))\n",
    "    print(\"FR LoRA :\", translate_sentence(s, lora_model, tokenizer))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e003f6",
   "metadata": {},
   "source": [
    "# **Traduction en sens inverse : FR --> EN**\n",
    "\n",
    "Après avoir fine-tuné T5 pour la traduction EN→FR, nous avons testé l’idée de simplement changer l’instruction pour obtenir l’inverse (FR→EN). Cependant, nous avons rapidement constaté que cela ne fonctionne pas : le modèle avait été entraîné uniquement dans un sens, avec des couples entrée→sortie bien définis, et il renvoyait donc systématiquement le même texte en sortie lorsqu’on lui demandait l’autre direction.\n",
    "\n",
    "Certaines architectures permettent effectivement la traduction bidirectionnelle, mais cela nécessite un entraînement dans les deux sens. Faute de temps et de ressources, nous avons donc entraîné un second modèle dédié au FR→EN, ce qui nous a permis au passage de mieux comprendre le comportement de T5 et ses limites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7275076",
   "metadata": {},
   "source": [
    "### **Choix des datasets**\n",
    "Pour entraîner le modèle FR→EN, nous sommes partis du constat que le modèle fine-tuné uniquement sur Europarl produisait de bonnes traductions pour les phrases longues et formelles, mais qu’il avait du mal avec les phrases simples ou plus informelles. \n",
    "\n",
    "En effet, Europarl correspond à un registre soutenu (discours parlementaires) et manque de variété.\n",
    "Pour corriger cela, nous avons donc mélangé trois datasets complémentaires :\n",
    "\n",
    "- Europarl (phrases longues, vocabulaire formel),\n",
    "\n",
    "- Opus Books (phrases littéraires, style varié),\n",
    "\n",
    "- Francophonia (phrases courtes, quotidiennes, beaucoup plus simples).\n",
    "\n",
    "Ce mix permet d’équilibrer le registre linguistique et de créer un dataset plus représentatif pour la traduction générale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4879ac",
   "metadata": {},
   "source": [
    "### **Phase préparatoire des données**\n",
    "\n",
    "Le script applique un pipeline complet :\n",
    "\n",
    "- Nettoyage des phrases (URLs, balises, caractères non imprimables, répétitions inutiles, etc.).\n",
    "\n",
    "- Filtrage (phrases identiques, code ou timestamps, mismatch de langue FR/EN).\n",
    "\n",
    "- Déduplication pour éviter d’entraîner plusieurs fois sur les mêmes couples.\n",
    "\n",
    "- Échantillonnage équilibré (123k Opus, 300k Europarl, 318k Francophonia).\n",
    "\n",
    "- Fusion et mélange des datasets.\n",
    "\n",
    "- Split train / validation / test (97% / 1.5% / 1.5%).\n",
    "\n",
    "- Sauvegarde des données nettoyées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ffa9e-38b9-4ad6-8590-2fbb2cc408f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bffe6ab12d64bf0878e31203f315a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a166a45071474e47952f1e1289d2cce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c4cb365b34de2b17fa4df088f1d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584f29e7d2294a8ea512008af51178e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81866d2491e94234a750a77906e7eb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bda59fbe4046fa948a74b855c03600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00000-of-00002.parquet:   0%|          | 0.00/193M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b497ed7964dd4738b95b90e59d0db16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00001-of-00002.parquet:   0%|          | 0.00/186M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11161a051f3b40b4bee1e9903709101a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2051014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3960283b319e40eba9d382a720ea2d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3cce007894479a332e4e991395474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be21e7fc3b304828aabe6feb209ff03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/31.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df56317f2eb64d1c8af0cf261189cab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/320162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db068f0fce33489ca65d762e3958399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opus_books: 127,085 paires avant nettoyage\n",
      "- europarl: 2,051,014 paires avant nettoyage\n",
      "- francophonia: 320,162 paires avant nettoyage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b98b1543ed04b80b6a5c2b864554e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalize::opus_books:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0984d0c750a48a5948dd4437ae85cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filter::opus_books:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> opus_books: 123,934 paires après nettoyage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c55116fba54783b913cc0bf09081ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalize::europarl:   0%|          | 0/2051014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b7cc9c4bff4af08d52e875470bad95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filter::europarl:   0%|          | 0/2051014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> europarl: 1,988,883 paires après nettoyage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae54bf896e5e41c3945564a8ae918462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalize::francophonia:   0%|          | 0/320162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa043f5a2b0f48a3a64afc5d44ab8d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filter::francophonia:   0%|          | 0/320162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> francophonia: 318,953 paires après nettoyage\n",
      "Tailles après cap:\n",
      "  opus_books: 123,934\n",
      "  europarl:   300,000\n",
      "  francophonia:318,953\n",
      "Taille totale après mix: 742887\n",
      "train 720600\n",
      "validation 11143\n",
      "test 11144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75351b7b6a7e4dc68a9cd94bc03fb93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/720600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6179a82fa1b94762b985c239a7c0f556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136fcd03323843f1ad6bac05e9f8d145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d582c5b9674b868016d0895e5a377b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f010767c4f4e4eae38a4bc496666d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e92856065ab475bb5c1aa5878fffe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu échantillon (validation) FR→EN:\n",
      "[europarl] FR: Nous avons d’ailleurs proposé des amendements qui rompent avec le principe du pays d’origine et excluent les services économiques d’intérêt général de son champ d’application.  ||  EN: We have, moreover, tabled some amendments that break with the country of origin principle and that exclude services of general economic interest from the scope of the directive.\n",
      "[francophonia] FR: Dans la vie de tous les jours nous avons beaucoup d'obligations et de responsabilités.  ||  EN: In everyday life we have many obligations and responsibilities.\n",
      "[europarl] FR: Pour pouvoir faire une véritable évaluation et obtenir des statistiques fiables, nous devons cependant définir une stratégie afin de déterminer quelles informations les États membres doivent communiquer à l'Observatoire.  ||  EN: In order to be able to carry out a credible assessment and produce reliable statistics, we must however have a strategy concerning what information the Member States are to send in to the Centre.\n",
      "[europarl] FR: Les besoins identifiés par le groupe van Miert ont été chiffrés à 600 milliards d'euros.  ||  EN: The requirements identified by the Van Miert Group have been estimated at EUR 600 billion.\n",
      "[francophonia] FR: l' inde est calme à l'automne, mais il est généralement humide en janvier.  ||  EN: india is quiet during fall, but it is usually wet in january.\n",
      "\n",
      "Terminé. Données prêtes dans: data_mt_fr_en\n"
     ]
    }
   ],
   "source": [
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED)\n",
    "\n",
    "OUT_DIR = \"data_mt_fr_en\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Chargement des datasets (remappés FR->EN)\n",
    "\n",
    "def _ensure_dataset(ds_or_dd: Dataset | DatasetDict) -> Dataset:\n",
    "    \"\"\"Convertit un DatasetDict en Dataset unique (concat de tous les splits).\"\"\"\n",
    "    if isinstance(ds_or_dd, DatasetDict):\n",
    "        parts = [v for k, v in ds_or_dd.items()]\n",
    "        if len(parts) == 1:\n",
    "            return parts[0]\n",
    "        return concatenate_datasets(parts)\n",
    "    return ds_or_dd\n",
    "\n",
    "def load_opus_books_fr_en() -> Dataset:\n",
    "    \"\"\"\n",
    "    opus_books config 'en-fr' -> on remappe FR->EN:\n",
    "      src_text = FR  (pair['fr'])\n",
    "      tgt_text = EN  (pair['en'])\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"opus_books\", \"en-fr\")  # souvent DatasetDict\n",
    "    ds = _ensure_dataset(ds)\n",
    "    ds = ds.rename_column(\"translation\", \"pair\")\n",
    "    ds = ds.map(lambda x: {\n",
    "        \"src_text\": x[\"pair\"].get(\"fr\", \"\"),   # FR\n",
    "        \"tgt_text\": x[\"pair\"].get(\"en\", \"\"),   # EN\n",
    "        \"source\": \"opus_books\"\n",
    "    }, remove_columns=[\"pair\"])\n",
    "    return ds\n",
    "\n",
    "def load_europarl_fr_en() -> Dataset:\n",
    "    \"\"\"\n",
    "    Europarl config 'en-fr' -> remap FR->EN\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\")\n",
    "    ds = ds.rename_column(\"translation\", \"pair\")\n",
    "    ds = ds.map(lambda x: {\n",
    "        \"src_text\": x[\"pair\"].get(\"fr\", \"\"),   # FR\n",
    "        \"tgt_text\": x[\"pair\"].get(\"en\", \"\"),   # EN\n",
    "        \"source\": \"europarl\"\n",
    "    }, remove_columns=[\"pair\"])\n",
    "    return ds\n",
    "\n",
    "def load_francophonia_fr_en() -> Dataset:\n",
    "    \"\"\"\n",
    "    FrancophonIA/english_french -> colonnes 'english', 'french'\n",
    "    On remappe FR->EN.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"FrancophonIA/english_french\", split=\"train\")\n",
    "    ds = ds.rename_column(\"french\", \"src_text\")   # FR\n",
    "    ds = ds.rename_column(\"english\", \"tgt_text\")  # EN\n",
    "    ds = ds.map(lambda x: {\"source\": \"francophonia\"})\n",
    "    return ds\n",
    "\n",
    "# 2) Pipeline de nettoyage\n",
    "\n",
    "_ws_re = re.compile(r\"\\s+\")\n",
    "_ctrl_re = re.compile(r\"[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]\")\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_tag_re = re.compile(r\"<[^>]+>\")\n",
    "_repeated_punct_re = re.compile(r\"([!?.,;:\\-–—])\\1{2,}\")\n",
    "_non_printable_re = re.compile(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E\\u00A0-\\uD7FF\\uE000-\\uFFFD]\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.replace(\"\\u00A0\", \" \")\n",
    "    text = _ctrl_re.sub(\" \", text)\n",
    "    text = _non_printable_re.sub(\" \", text)\n",
    "    text = _tag_re.sub(\" \", text)\n",
    "    text = _url_re.sub(\" \", text)\n",
    "    text = _repeated_punct_re.sub(r\"\\1\", text)\n",
    "    text = _ws_re.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def ascii_ratio(s: str) -> float:\n",
    "    if not s:\n",
    "        return 1.0\n",
    "    ascii_chars = sum(1 for ch in s if ord(ch) < 128)\n",
    "    return ascii_chars / max(1, len(s))\n",
    "\n",
    "def punctuation_ratio(s: str) -> float:\n",
    "    if not s:\n",
    "        return 0.0\n",
    "    punct = sum(1 for ch in s if ch in \".,;:!?…\\\"'()[]{}<>/\\\\|@#$%^&*_+=~`\")\n",
    "    return punct / max(1, len(s))\n",
    "\n",
    "def looks_code_like(s: str) -> bool:\n",
    "    patterns = [\n",
    "        r\"^\\d{1,2}:\\d{2}(:\\d{2})?$\",    # 01:23:45\n",
    "        r\"^\\[\\w+\\]$\",                   # [MUSIC], [LAUGH]\n",
    "        r\"\\b(function|var|let|const|<script|</script>)\\b\",\n",
    "        r\"{\\s*}$\", r\";\\s*$\"\n",
    "    ]\n",
    "    return any(re.search(p, s) for p in patterns)\n",
    "\n",
    "def length_ok(src: str, tgt: str,\n",
    "              min_len=1, max_len=200,\n",
    "              max_len_ratio=3.0) -> bool:\n",
    "    ls, lt = len(src.split()), len(tgt.split())\n",
    "    if ls < min_len or lt < min_len:\n",
    "        return False\n",
    "    if ls > max_len or lt > max_len:\n",
    "        return False\n",
    "    ratio = max(ls, lt) / max(1, min(ls, lt))\n",
    "    return ratio <= max_len_ratio\n",
    "\n",
    "def lang_ok(src: str, tgt: str) -> bool:\n",
    "    \"\"\"FR -> EN attendu.\"\"\"\n",
    "    if not HAS_LANGID:\n",
    "        return True\n",
    "    s_lang, s_conf = langid.classify(src)  # should be fr\n",
    "    t_lang, t_conf = langid.classify(tgt)  # should be en\n",
    "    return (s_lang == \"fr\" and t_lang == \"en\" and s_conf > 0.85 and t_conf > 0.85)\n",
    "\n",
    "def clean_example(ex: Dict) -> Dict:\n",
    "    s = normalize(ex[\"src_text\"])\n",
    "    t = normalize(ex[\"tgt_text\"])\n",
    "    return {\"src_text\": s, \"tgt_text\": t, \"source\": ex[\"source\"]}\n",
    "\n",
    "def filter_example(ex: Dict) -> bool:\n",
    "    s = ex[\"src_text\"]; t = ex[\"tgt_text\"]\n",
    "    if not s or not t:\n",
    "        return False\n",
    "    if s == t:\n",
    "        return False\n",
    "    if looks_code_like(s) or looks_code_like(t):\n",
    "        return False\n",
    "    if punctuation_ratio(s) > 0.35 or punctuation_ratio(t) > 0.35:\n",
    "        return False\n",
    "    # Pour FR->EN: on vérifie l'ASCII sur la cible anglaise (t)\n",
    "    if ascii_ratio(t) < 0.75:\n",
    "        return False\n",
    "    if not length_ok(s, t):\n",
    "        return False\n",
    "    if not lang_ok(s, t):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def dedupe(ds: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Déduplication (src_text, tgt_text), compatible anciennes versions.\n",
    "    \"\"\"\n",
    "    if hasattr(ds, \"drop_duplicates\"):\n",
    "        try:\n",
    "            return ds.drop_duplicates(subset=[\"src_text\", \"tgt_text\"])\n",
    "        except TypeError:\n",
    "            return ds.drop_duplicates(column_names=[\"src_text\", \"tgt_text\"])\n",
    "    \n",
    "    df = ds.to_pandas()\n",
    "    df = df.drop_duplicates(subset=[\"src_text\", \"tgt_text\"]).reset_index(drop=True)\n",
    "    return Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "\n",
    "# 3) Chargement + nettoyage\n",
    "\n",
    "print(\"Chargement des datasets...\")\n",
    "opus_books = load_opus_books_fr_en()\n",
    "europarl   = load_europarl_fr_en()\n",
    "franco     = load_francophonia_fr_en()\n",
    "\n",
    "for name, ds in [(\"opus_books\", opus_books), (\"europarl\", europarl), (\"francophonia\", franco)]:\n",
    "    print(f\"- {name}: {len(ds):,} paires avant nettoyage\")\n",
    "\n",
    "def apply_cleaning(ds: Dataset, name: str) -> Dataset:\n",
    "    ds = ds.map(clean_example, desc=f\"normalize::{name}\")\n",
    "    ds = ds.filter(filter_example, desc=f\"filter::{name}\")\n",
    "    ds = dedupe(ds)\n",
    "    print(f\"  -> {name}: {len(ds):,} paires après nettoyage\")\n",
    "    return ds\n",
    "\n",
    "opus_books = apply_cleaning(opus_books, \"opus_books\")\n",
    "europarl   = apply_cleaning(europarl,   \"europarl\")\n",
    "franco     = apply_cleaning(franco,     \"francophonia\")\n",
    "\n",
    "\n",
    "# 4) Échantillonnage & mix\n",
    "\n",
    "TARGET_MAX = {\n",
    "    \"opus_books\": 300_000,\n",
    "    \"europarl\":   300_000,\n",
    "    \"francophonia\": 400_000\n",
    "}\n",
    "\n",
    "def cap(ds: Dataset, cap_size: int, seed=RNG_SEED) -> Dataset:\n",
    "    if len(ds) <= cap_size:\n",
    "        return ds\n",
    "    idx = list(range(len(ds)))\n",
    "    random.Random(seed).shuffle(idx)\n",
    "    return ds.select(idx[:cap_size])\n",
    "\n",
    "opus_cap = cap(opus_books, TARGET_MAX[\"opus_books\"])\n",
    "euro_cap = cap(europarl,   TARGET_MAX[\"europarl\"])\n",
    "fran_cap = cap(franco,     TARGET_MAX[\"francophonia\"])\n",
    "\n",
    "print(\"Tailles après cap:\")\n",
    "print(f\"  opus_books: {len(opus_cap):,}\")\n",
    "print(f\"  europarl:   {len(euro_cap):,}\")\n",
    "print(f\"  francophonia:{len(fran_cap):,}\")\n",
    "\n",
    "# Mix (pondération légère possible)\n",
    "mix_parts = [opus_cap, fran_cap, euro_cap]\n",
    "mix = concatenate_datasets(mix_parts).shuffle(seed=RNG_SEED)\n",
    "print(\"Taille totale après mix:\", len(mix))\n",
    "\n",
    "\n",
    "# 5) Split train/validation/test\n",
    "\n",
    "def split_dataset(ds: Dataset,\n",
    "                  train_ratio=0.97,\n",
    "                  valid_ratio=0.015,\n",
    "                  seed=RNG_SEED) -> DatasetDict:\n",
    "    n = len(ds)\n",
    "    idx = list(range(n))\n",
    "    random.Random(seed).shuffle(idx)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_valid = int(n * valid_ratio)\n",
    "    train_idx = idx[:n_train]\n",
    "    valid_idx = idx[n_train:n_train + n_valid]\n",
    "    test_idx  = idx[n_train + n_valid:]\n",
    "    return DatasetDict({\n",
    "        \"train\": ds.select(train_idx),\n",
    "        \"validation\": ds.select(valid_idx),\n",
    "        \"test\": ds.select(test_idx)\n",
    "    })\n",
    "\n",
    "dsdict = split_dataset(mix)\n",
    "for split in dsdict:\n",
    "    print(split, len(dsdict[split]))\n",
    "\n",
    "# 6) Sauvegarde (Arrow/Parquet + JSONL)\n",
    "\n",
    "ARROW_DIR   = os.path.join(OUT_DIR, \"arrow\")\n",
    "PARQUET_DIR = os.path.join(OUT_DIR, \"parquet\")\n",
    "JSONL_DIR   = os.path.join(OUT_DIR, \"jsonl\")\n",
    "os.makedirs(ARROW_DIR, exist_ok=True)\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "os.makedirs(JSONL_DIR, exist_ok=True)\n",
    "\n",
    "dsdict.save_to_disk(ARROW_DIR)\n",
    "\n",
    "for split, ds in dsdict.items():\n",
    "    ds.to_parquet(os.path.join(PARQUET_DIR, f\"{split}.parquet\"))\n",
    "\n",
    "def export_jsonl(ds: Dataset, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds:\n",
    "            f.write(json.dumps(\n",
    "                {\"src\": ex[\"src_text\"], \"tgt\": ex[\"tgt_text\"], \"source\": ex[\"source\"]},\n",
    "                ensure_ascii=False\n",
    "            ) + \"\\n\")\n",
    "\n",
    "for split, ds in dsdict.items():\n",
    "    export_jsonl(ds, os.path.join(JSONL_DIR, f\"{split}.jsonl\"))\n",
    "\n",
    "def sample_preview(ds: Dataset, k=5):\n",
    "    k = min(k, len(ds))\n",
    "    ids = random.sample(range(len(ds)), k) if k > 0 else []\n",
    "    return [ds[i] for i in ids]\n",
    "\n",
    "print(\"\\nAperçu échantillon (validation) FR→EN:\")\n",
    "for ex in sample_preview(dsdict[\"validation\"], 5):\n",
    "    print(f\"[{ex['source']}] FR: {ex['src_text']}  ||  EN: {ex['tgt_text']}\")\n",
    "\n",
    "print(\"\\nTerminé. Données prêtes dans:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c7e58",
   "metadata": {},
   "source": [
    "### **Entrainement et Evaluation du modèle**\n",
    "Ici, on entraîne le modèle puis on l'évalue sur des données démo.\n",
    "On affiche sa loss tout au long de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76524db9-cfa3-4bf8-9088-3aceeda41b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] Device: cuda\n",
      "[TRAIN] Chargement du DatasetDict...\n",
      "{'train': 720600, 'validation': 11143, 'test': 11144}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ceda6af803c4ba793d20d5fa75d3b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b904b8ee34a27b102268549da03b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c60330587ca40c1b65872a7ee46b813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7639f5c39266475d95f1e522ff66b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905b272038da4aa08889632f6ff5f6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ede09b4df4e3d9d955fb0c78bb27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Tokenisation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e7fb69a41846ce9dc8b98b2bfa5cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize::train:   0%|          | 0/720600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f9c7294d7b478a8eb1a34718bdf4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize::validation:   0%|          | 0/11143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aec4c7f6744111b95858f088b52873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize::test:   0%|          | 0/11144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169405ada90c41c08cc6350f37086911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb70fc8a2a45c2ba02db51ff6d5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Transformers version: 4.57.3\n",
      "⚠️  Mode LEGACY activé : évaluation intégrée indisponible, on évaluera après l'entraînement.\n",
      "[TRAIN] Démarrage de l'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_478/2118881286.py:154: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67557' max='67557' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67557/67557 1:27:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>1.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.400100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>1.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>1.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>1.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>1.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>1.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>1.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>1.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>1.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>1.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>1.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>1.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>1.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>1.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>1.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>1.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>1.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>1.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>1.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>1.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>1.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>1.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>1.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>1.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>1.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>1.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>1.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>1.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>1.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>1.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>1.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>1.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>1.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>1.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>1.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>1.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>1.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>1.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>1.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>1.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>1.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>1.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>1.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>1.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>1.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>1.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>1.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>1.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>1.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>1.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>1.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>1.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>1.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>1.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>1.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>1.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>1.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>1.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>1.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>1.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>1.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>1.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>1.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>1.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>1.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>1.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>1.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>1.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>1.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>1.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>1.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>1.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>1.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>1.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>1.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>1.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>1.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>1.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>1.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>1.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>1.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>1.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>1.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>1.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>1.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>1.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>1.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>1.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>1.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>1.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>1.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>1.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>1.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>1.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>1.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>1.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>1.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>1.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>1.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>1.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>1.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>1.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>1.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>1.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50600</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50800</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51400</td>\n",
       "      <td>1.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>1.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51800</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52400</td>\n",
       "      <td>1.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52600</td>\n",
       "      <td>1.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53200</td>\n",
       "      <td>1.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53600</td>\n",
       "      <td>1.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53800</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54200</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54400</td>\n",
       "      <td>1.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54600</td>\n",
       "      <td>1.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54800</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>1.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55400</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55600</td>\n",
       "      <td>1.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55800</td>\n",
       "      <td>1.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56200</td>\n",
       "      <td>1.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56400</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56600</td>\n",
       "      <td>1.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56800</td>\n",
       "      <td>1.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57200</td>\n",
       "      <td>1.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57400</td>\n",
       "      <td>1.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57600</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57800</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58200</td>\n",
       "      <td>1.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58400</td>\n",
       "      <td>1.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58600</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58800</td>\n",
       "      <td>1.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59200</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59400</td>\n",
       "      <td>1.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59600</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59800</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60200</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60400</td>\n",
       "      <td>1.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60600</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60800</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61200</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61400</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61600</td>\n",
       "      <td>1.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61800</td>\n",
       "      <td>1.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62200</td>\n",
       "      <td>1.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62400</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62600</td>\n",
       "      <td>1.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62800</td>\n",
       "      <td>1.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63200</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63400</td>\n",
       "      <td>1.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63600</td>\n",
       "      <td>1.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63800</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64200</td>\n",
       "      <td>1.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64400</td>\n",
       "      <td>1.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64600</td>\n",
       "      <td>1.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64800</td>\n",
       "      <td>1.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65200</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65400</td>\n",
       "      <td>1.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65600</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65800</td>\n",
       "      <td>1.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66200</td>\n",
       "      <td>1.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66400</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66600</td>\n",
       "      <td>1.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66800</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67200</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67400</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Validation (manuel)...\n",
      "Validation: {'bleu': 41.6465, 'chrf': 62.2474}\n",
      "[EVAL] Test (manuel)...\n",
      "Test: {'bleu': 41.8978, 'chrf': 62.5291}\n",
      "\n",
      "Traductions de démonstration (FR→EN):\n",
      "FR: Je t'apprécie beaucoup.\n",
      "EN: I appreciate you a lot.\n",
      "\n",
      "FR: Où se trouve la gare la plus proche ?\n",
      "EN: Where is the nearest train station?\n",
      "\n",
      "FR: Ce livre a été écrit par un scientifique renommé.\n",
      "EN: This book was written by a renamed scientist.\n",
      "\n",
      "FR: Pourrais-tu m'aider sur ce projet ?\n",
      "EN: Could you help me with this project?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraînement T5-small FR→EN\n",
    "DATA_DIR = \"data_mt_fr_en/arrow\"     \n",
    "MODEL_NAME = \"t5-small\"\n",
    "OUT_DIR = \"outputs/t5-fren\"\n",
    "\n",
    "SEED = 42\n",
    "MAX_SOURCE_LEN = 128\n",
    "MAX_TARGET_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM = 2\n",
    "LR = 3e-4\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.06\n",
    "LABEL_SMOOTHING = 0.1\n",
    "BEAM_SIZE = 4\n",
    "GEN_MAX_NEW_TOKENS = 128\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n[TRAIN] Device: {device}\")\n",
    "\n",
    "print(\"[TRAIN] Chargement du DatasetDict...\")\n",
    "dsdict = load_from_disk(DATA_DIR)\n",
    "print({k: len(v) for k, v in dsdict.items()})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "PREFIX = \"translate French to English: \"\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    # FR->EN : input = FR (src_text), target = EN (tgt_text)\n",
    "    inputs = [PREFIX + s for s in batch[\"src_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_SOURCE_LEN, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"tgt_text\"], max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"[TRAIN] Tokenisation...\")\n",
    "tokenized = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    if split in dsdict:\n",
    "        tokenized[split] = dsdict[split].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dsdict[split].column_names,\n",
    "            desc=f\"tokenize::{split}\"\n",
    "        )\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def postprocess_text(preds: List[str], labels: List[str]):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, [[l] for l in labels]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf = metric_chrf.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels])\n",
    "    gen_lens = [ (pred != \"\" and len(tokenizer(pred).input_ids)) or 0 for pred in decoded_preds ]\n",
    "    return {\"bleu\": round(bleu[\"score\"], 4), \"chrf\": round(chrf[\"score\"], 4),\n",
    "            \"gen_len\": round(sum(gen_lens) / max(1, len(gen_lens)), 2)}\n",
    "\n",
    "# 6) Entraînement (compat toutes versions)\n",
    "\n",
    "TRANSF_VER = version.parse(transformers.__version__)\n",
    "print(\"[TRAIN] Transformers version:\", TRANSF_VER)\n",
    "LEGACY = False\n",
    "\n",
    "common_kwargs = dict(\n",
    "    output_dir=OUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    "    logging_steps=200,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        logging_strategy=\"steps\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=GEN_MAX_NEW_TOKENS,\n",
    "        generation_num_beams=BEAM_SIZE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        bf16=False,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "except TypeError:\n",
    "    LEGACY = True\n",
    "    print(\"⚠️  Mode LEGACY activé : évaluation intégrée indisponible, on évaluera après l'entraînement.\")\n",
    "    training_args = Seq2SeqTrainingArguments(**common_kwargs)\n",
    "\n",
    "callbacks = []\n",
    "try:\n",
    "    \n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "except Exception:\n",
    "    callbacks = []\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=None if LEGACY else tokenized.get(\"validation\"),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None if LEGACY else compute_metrics,\n",
    "    callbacks=callbacks if not LEGACY else None\n",
    ")\n",
    "\n",
    "print(\"[TRAIN] Démarrage de l'entraînement...\")\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegardes\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# 7) Évaluation\n",
    "def generate_translate(batch_texts, max_new_tokens=GEN_MAX_NEW_TOKENS, num_beams=BEAM_SIZE):\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_beams)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def eval_split_manual(split_ds):\n",
    "    preds, refs = [], []\n",
    "    B = 64\n",
    "    buf_src = []\n",
    "    for ex in split_ds:\n",
    "        buf_src.append(PREFIX + ex[\"src_text\"])  # FR input (avec préfixe)\n",
    "        refs.append(ex[\"tgt_text\"])              # EN ref\n",
    "        if len(buf_src) >= B:\n",
    "            preds += generate_translate(buf_src)\n",
    "            buf_src = []\n",
    "    if buf_src:\n",
    "        preds += generate_translate(buf_src)\n",
    "    bleu = metric_bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
    "    chrf = metric_chrf.compute(predictions=preds, references=refs)\n",
    "    return {\"bleu\": round(bleu[\"score\"], 4), \"chrf\": round(chrf[\"score\"], 4)}\n",
    "\n",
    "if LEGACY:\n",
    "    print(\"[EVAL] Validation (manuel)...\")\n",
    "    metrics_val = eval_split_manual(dsdict[\"validation\"])\n",
    "    print(\"Validation:\", metrics_val)\n",
    "    if \"test\" in dsdict:\n",
    "        print(\"[EVAL] Test (manuel)...\")\n",
    "        metrics_test = eval_split_manual(dsdict[\"test\"])\n",
    "        print(\"Test:\", metrics_test)\n",
    "else:\n",
    "    print(\"[EVAL] Validation (intégrée)...\")\n",
    "    metrics_val = trainer.evaluate(max_length=GEN_MAX_NEW_TOKENS, num_beams=BEAM_SIZE)\n",
    "    metrics_val[\"perplexity\"] = math.exp(metrics_val[\"eval_loss\"]) if metrics_val.get(\"eval_loss\", 99) < 20 else float(\"inf\")\n",
    "    print(metrics_val)\n",
    "    if \"test\" in tokenized:\n",
    "        print(\"[EVAL] Test (intégrée)...\")\n",
    "        test_metrics = trainer.evaluate(\n",
    "            eval_dataset=tokenized[\"test\"],\n",
    "            max_length=GEN_MAX_NEW_TOKENS,\n",
    "            num_beams=BEAM_SIZE,\n",
    "            metric_key_prefix=\"test\"\n",
    "        )\n",
    "        test_metrics[\"test_perplexity\"] = math.exp(test_metrics[\"test_loss\"]) if test_metrics.get(\"test_loss\", 99) < 20 else float(\"inf\")\n",
    "        print(test_metrics)\n",
    "\n",
    "\n",
    "# 8) Démo FR->EN\n",
    "SAMPLES_FR = [\n",
    "    \"Je t'apprécie beaucoup.\",\n",
    "    \"Où se trouve la gare la plus proche ?\",\n",
    "    \"Ce livre a été écrit par un scientifique renommé.\",\n",
    "    \"Pourrais-tu m'aider sur ce projet ?\",\n",
    "]\n",
    "print(\"\\nTraductions de démonstration (FR→EN):\")\n",
    "for s in SAMPLES_FR:\n",
    "    out = generate_translate([PREFIX + s])[0]\n",
    "    print(f\"FR: {s}\\nEN: {out}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d61f84",
   "metadata": {},
   "source": [
    "### **Commentaire**\n",
    "Le modèle FR→EN obtient un BLEU d’environ 41.8 et un CHRF d’environ 62.5, ce qui indique une bonne qualité de traduction globale. Sur les exemples manuels, les phrases simples sont bien traduites et quasi bien reformulées en anglais naturel.\n",
    "\n",
    "On note toutefois une petite erreur sur renommé → renamed, ce qui montre que le modèle reste sensible à certains faux-amis, mais la structure et le sens général demeurent corrects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23c166",
   "metadata": {},
   "source": [
    "# **Conclusion / limites /perspectives / acquis d'apprentissage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb41555-4dfb-4b8f-b2eb-22041919f128",
   "metadata": {},
   "source": [
    "### Limites\n",
    "\n",
    "Malgré ces résultats encourageants, notre approche présente plusieurs **limites** :\n",
    "\n",
    "- **Spécialisation des modèles**  \n",
    "  Chaque modèle est entraîné dans **une seule direction**. Il est donc nécessaire de maintenir **deux modèles séparés** pour couvrir EN->FR et FR->EN, ce qui complique un peu le déploiement. Cela a été fait ainsi par manque de temps, mais un seul modèle T5 aurait pu être entraîné pour la traduction bidirectionnelle, en lui fournissant des exemples dans les deux sens (EN->FR et FR->EN) avec une indication explicite de la direction souhaitée.\n",
    "\n",
    "- **Domaine des données (EN->FR)**  \n",
    "  Le modèle EN->FR est entraîné quasi exclusivement sur **Europarl**, donc exposé surtout à un **registre politique formel**. Sa généralisation à des textes informels, techniques, ou très spécialisés reste incertaine.\n",
    "\n",
    "- **Qualité et variété des données (FR->EN)**  \n",
    "  Même si le corpus FR->EN est plus varié (Europarl + Opus Books + Francophonia), il reste constitué de phrases en général propres et bien écrites. La robustesse face à du **langage bruité** (fautes, réseaux sociaux, etc.) n’a pas été évaluée.\n",
    "\n",
    "- **Longueur des séquences**  \n",
    "  Nous avons travaillé avec des **longueurs maximales d’entrée/sortie bornées** (par exemple 128 tokens à la génération). \n",
    "\n",
    "- **Adaptation partielle avec LoRA**  \n",
    "  LoRA réduit fortement le nombre de paramètres entraînables, ce qui est très intéressant en pratique, simplifie cette tache du fait que l'on ne risque pas de \"détruire\" le modèle mais peut aussi **limiter les gains \"ultimes\"** par rapport à un fine-tuning complet.\n",
    "\n",
    "- **Évaluation surtout automatique**  \n",
    "  Nous nous appuyons principalement sur des **métriques automatiques** (BLEU, METEOR, ROUGE, chrF). Et nous avons uniquement regarder manuellement que quelques phrases. Nous n’avons donc pas réalisé de véritable **évaluation humaine systématique**, ce qui limite la mesure de la qualité perçue par des utilisateurs finaux. **L'usage prévaut sur la règle**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de07ee3-cc84-48b8-9ff9-a2465ba4884f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Perspectives d’amélioration\n",
    "\n",
    "Plusieurs **pistes** peuvent être explorées pour aller plus loin :\n",
    "\n",
    "- **Vers un modèle réellement bidirectionnel**  \n",
    "  En s’inspirant du paradigme T5 “text-to-text”, on pourrait entraîner **un seul modèle** de manière à mutualiser les capacités et simplifier le déploiement.\n",
    "\n",
    "- **Enrichissement et diversification des corpus**  \n",
    "  Ajouter des données de domaines variés (presse, dialogues, textes techniques, données bruitées) permettrait de rendre les modèles **plus robustes** et plus utilisables (meilleur) dans des situations réelles.\n",
    "\n",
    "- **Exploration d’autres méthodes**  \n",
    "  Tester d’autres approches que LoRA (prefix-tuning, adapters, etc.) pourrait permettre de trouver un meilleur compromis entre **nombre de paramètres entraînables**, **stabilité** et **performance**.\n",
    "\n",
    "- **Affinage des hyperparamètres et du décodage**  \n",
    "  Une recherche plus fine sur le **learning rate**, le **nombre d’épochs**, le **rang LoRA**, ainsi que sur les paramètres de génération (*num_beams*, *length_penalty*, *no_repeat_ngram_size*, etc.) pourrait encore améliorer les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262c7fb-8f7b-449f-a64b-f8baec528591",
   "metadata": {},
   "source": [
    "### Acquis d’apprentissage\n",
    "\n",
    "Ce projet nous a permis d’acquérir et de consolider plusieurs **compétences clés** :\n",
    "\n",
    "- Mise en place d’une **pipeline de traduction de bout en bout** : chargement des données (datasets HuggingFace), prétraitement, split train/validation/test, entraînement, évaluation et tests manuels.\n",
    "\n",
    "- Compréhension pratique du modèle **T5** et du paradigme **“text-to-text”**, avec la gestion de l’encodage, des labels, et de la valeur `-100` pour ignorer certains tokens dans la perte.\n",
    "\n",
    "- Maîtrise de l’**adaptation par LoRA** via la librairie PEFT : configuration (TaskType *SEQ_2_SEQ_LM*, *r*, *lora_alpha*, *lora_dropout*), impact sur le nombre de paramètres entraînables et sur la consommation mémoire.\n",
    "\n",
    "- Capacité à **gérer les ressources** (GPU/RAM) : utilisation de *gradient_accumulation_steps*, d’un callback de garbage collector, de batchs adaptés et d’un échantillonnage pour travailler sur plusieurs centaines de milliers de paires de phrases.\n",
    "\n",
    "- Utilisation et interprétation de **métriques de traduction** (BLEU, METEOR, ROUGE, chrF), avec une compréhension de ce que chacune mesure et de leurs limites.\n",
    "\n",
    "En résumé, nous avons construit **deux modèles de traduction spécialisés et complémentaires**, tout en explorant des techniques d’adaptation légère. Ce travail nous a donné une vision concrète des compromis entre **qualité de traduction**, **quantité et nature des données** et **contraintes de calcul**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

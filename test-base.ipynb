{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb6a9f41-64d6-4976-a9fc-b9e00aa3c7f9",
   "metadata": {},
   "source": [
    "## Membres du Groupe de Projet\n",
    "\n",
    "- Maxence KAMIONKA\n",
    "- Mikhaïl BENALI\n",
    "- Hadja BAH\n",
    "- Emmanuel DAGNOGO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54cec6d-7770-48d1-9816-0a67035b2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd78f78-71ef-42b0-ac1c-869ca9b16332",
   "metadata": {},
   "source": [
    "Ici nous importons PyTorch afin de détécter si un GPU est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1790e60-435d-47f2-80f1-a2e371c9f961",
   "metadata": {},
   "source": [
    "Nous importons également gc, un garbage collector, permettant de vider la mémoire RAM d'éléments inutiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f9ae-8932-4f97-b23c-ce6df94c4421",
   "metadata": {},
   "source": [
    "## Test du modèle de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e2170-2f8b-4f9c-8053-5e8d0d30a311",
   "metadata": {},
   "source": [
    "Nous sommes partis du modèle T5-base disponible via la bibliothèque transformers de HuggingFace et avons utilisé le tokenizer associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e092c15-9e28-428f-85b2-c5e1c5ab8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"t5-base\"\n",
    "INSTRUCTION = \"translate English to French: \"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7e6434-1fcb-4810-9a7c-46782fb9850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7ed64-717b-4ff0-b732-dda9fd992d4c",
   "metadata": {},
   "source": [
    "## Utilisation de LoRa (Low-Rank Adaption)\n",
    "\n",
    "On utilise LoRA afin d'entrainer des paramètres extra tout en gelant ceux du modèle de base pour ne pas lui faire oublier ce qu'il sait déjà faire originellement. On tune alors ces poids supplémentaires plutôt que tout le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d372ab3b-4cec-44a2-8c4f-d9ea73344231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "def load_lora_model():\n",
    "    # on réutilise MODEL_NAME & tokenizer définis plus haut\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "OPUS avait été considéré plus tôt mais les traductions un peu \"étranges\" remarquées ont fait que le dataset Europarl a été privilégié à la fin.\n",
    "\n",
    "\n",
    "**Europarl en-fr** : phrases longues, style plus formel, discours parlementaires -> plus de contexte par phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "source": [
    "**On préfèrera augmenter le nombre de données (ici 700K avec 0.5% en val) plutot qu'augmenter le nombre d'epochs (ici 2) pour éviter le sur-apprentissage mais aussi que le modèle généralise mieux en voyant davantage de phrases, tournures et vocabulaire.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data_old(sample_size_train=80000, sample_size_val=1000):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def load_data(sample_size_train=700000, sample_size_val=3500):\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "    \n",
    "    return euro_train, euro_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "EN : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "FR : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "--- val example 1 ---\n",
      "EN : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "FR : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- val example 2 ---\n",
      "EN : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "FR : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données brutes\n",
    "train_ds, val_ds = load_data()\n",
    "\n",
    "def show_raw_example(ds, idx=0, prefix=\"train\"):\n",
    "    ex = ds[idx]\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"EN :\", ex[\"translation\"][\"en\"])\n",
    "    print(\"FR :\", ex[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "# 3 exemples du train et 3 de la val\n",
    "for i in range(3):\n",
    "    show_raw_example(train_ds, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_raw_example(val_ds, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # Encodage des entrées\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encodage des cibles\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # ignorer les PAD dans la loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ffe0be-9733-4bcd-b503-e03520f3c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 223,788,288 || trainable%: 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                           | 0/700000 [00:00<?, ? examples/s]C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████████████████████████████████████████████████████| 700000/700000 [01:28<00:00, 7885.46 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 3500/3500 [00:00<00:00, 8235.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Charger modèle LoRA & réutiliser le tokenizer global\n",
    "model = load_lora_model()\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51ce2e-1a93-428b-bf34-6e2ffe6a71ff",
   "metadata": {},
   "source": [
    "Voici quelques exemples montrant les embeddings associés aux inputs et labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f5ed41-0751-4860-aabb-b62d122c6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_example(raw_ds, tokenized_ds, idx=0, prefix=\"train\"):\n",
    "    raw = raw_ds[idx]\n",
    "    tok = tokenized_ds[idx]\n",
    "\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"RAW EN :\", raw[\"translation\"][\"en\"])\n",
    "    print(\"RAW FR :\", raw[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "    # Inputs\n",
    "    print(\"input_ids[:20] :\", tok[\"input_ids\"][:20])\n",
    "    print(\"Decoded input  :\", tokenizer.decode(tok[\"input_ids\"], skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "    # Labels (enlevant les -100 pour re-décoder)\n",
    "    labels = tok[\"labels\"]\n",
    "    # On remplace les -100 par pad_token_id pour pouvoir décoder\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels_for_decode = [pad_id if x == -100 else x for x in labels]\n",
    "    print(\"labels[:20]     :\", labels[:20])\n",
    "    print(\"Decoded labels  :\", tokenizer.decode(labels_for_decode, skip_special_tokens=True))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1ad48a-590c-4800-b266-50f560637202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "RAW EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "RAW FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 282, 1611, 7, 6, 28, 69, 351, 6, 69, 1543, 13, 3065, 11, 69, 1456]\n",
      "Decoded input  : translate English to French: As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "\n",
      "labels[:20]     : [622, 2430, 3890, 35, 7, 238, 678, 7056, 6, 393, 1089, 11183, 6, 1089, 1543, 20, 50, 25060, 3, 15]\n",
      "Decoded labels  : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "RAW EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "RAW FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 94, 405, 5071, 2516, 21, 1402, 24, 273, 113, 33, 300, 8, 3, 22469, 953]\n",
      "Decoded input  : translate English to French: It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "\n",
      "labels[:20]     : [802, 259, 3, 26, 31, 6752, 29500, 238, 110, 3787, 285, 21442, 29, 17, 3, 8934, 20, 50, 953, 20]\n",
      "Decoded labels  : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "RAW EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "RAW FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 101, 17516, 857, 24, 8, 12326, 19, 1345, 1879, 250, 34, 11708, 7, 3, 9]\n",
      "Decoded input  : translate English to French: We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "\n",
      "labels[:20]     : [3, 18990, 6, 678, 12182, 106, 7, 238, 90, 29296, 259, 247, 3, 40, 22, 7408, 19185, 7, 288, 6]\n",
      "Decoded labels  : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "RAW EN : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "RAW FR : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 531, 25, 1845, 6, 14595, 6, 24, 125, 62, 33, 5008, 16, 69, 712, 512]\n",
      "Decoded input  : translate English to French: Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "\n",
      "labels[:20]     : [41, 5332, 61, 312, 26864, 7, 2378, 3, 9, 26, 3493, 18, 173, 238, 247, 1680, 6183, 775, 6, 678]\n",
      "Decoded labels  : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "\n",
      "--- val example 1 ---\n",
      "RAW EN : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "RAW FR : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 304, 369, 223, 12, 82, 711, 500, 6, 27, 317, 24, 8, 1880, 35, 729]\n",
      "Decoded input  : translate English to French: To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "\n",
      "labels[:20]     : [695, 10881, 288, 3, 922, 260, 35, 29875, 6, 528, 9790, 238, 90, 3, 287, 1257, 3, 25428, 99, 1880]\n",
      "Decoded labels  : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- val example 2 ---\n",
      "RAW EN : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "RAW FR : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 24624, 6, 3, 9, 7234, 3527, 133, 163, 4368, 552, 8, 414, 13, 8, 215]\n",
      "Decoded input  : translate English to French: Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "\n",
      "labels[:20]     : [7913, 40, 3335, 6, 50, 3527, 813, 3466, 6534, 3, 29, 15, 14770, 9, 238, 4007, 31, 85, 50, 2202]\n",
      "Decoded labels  : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_processed_example(train_ds, tokenized_train, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_processed_example(val_ds, tokenized_val, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ccd77-839a-42f7-9dec-0bbb7065af53",
   "metadata": {},
   "source": [
    "Nous avons utilisé 3 métriques afin de mesurer les performances du modèle : \n",
    "\n",
    "La métrique BLEU permet de comparer des n-grammes entre la prédiction et le label afin de mesurer la proximité de la réponse générée avec les différentes réponses attendues.\n",
    "\n",
    "Le ROUGE est principalement utilisé pour évaluer la qualité des résumés automatiques. Il mesure le recall des n-grammes entre le résumé généré et un ou plusieurs résumés de référence. ROUGE-1 évalue les mots individuels, tandis que ROUGE-2 évalue les paires de mots. Un score ROUGE élevé indique que le résumé couvre bien les informations clés du texte original.\n",
    "\n",
    "Le METEOR est une métrique plus flexible que BLEU, car elle prend en compte non seulement la précision des mots, mais aussi les synonymes et leur ordre. Elle utilise des techniques de recall et de précision, ainsi qu’un alignement entre les mots du texte généré et ceux de référence. METEOR est souvent utilisé pour évaluer les traductions et les résumés, parce qu'il offre une évaluation plus nuancée de la qualité linguistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    # eval_preds peut être un tuple (preds, labels)\n",
    "    # ou un objet EvalPrediction avec .predictions et .label_ids\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement le 1er élément\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # On met tout en np.array pour être tranquille\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    # Cas où preds = logits (batch, seq_len, vocab_size) -> on prend l'argmax\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # On s'assure que ce sont bien des entiers\n",
    "    preds = preds.astype(\"int64\")\n",
    "\n",
    "    # si jamais il y a des valeurs négatives dans preds on les remplace par pad_token_id avant decode\n",
    "    preds[preds < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Gestion des labels : on remet pad_token_id à la place des -100 pour décoder\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode des prédictions et des labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33188-476e-43b6-81a8-34394241864b",
   "metadata": {},
   "source": [
    "## Métriques sur modèle de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20f70c-583d-4c23-9b93-0de3ef1ca176",
   "metadata": {},
   "source": [
    "Nous avons d'abord mesuré les métriques sur le modèle de base afin de comparer les deux et constater une baisse ou évolution des performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b37d9e-9dea-4adc-8bbf-5e6ce0783d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_38968\\32419789.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Seq2SeqTrainer(\n",
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\generation\\utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='438' max='438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [438/438 12:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics :\n",
      "{'eval_bleu': 37.108577481672434,\n",
      " 'eval_loss': 0.9090045094490051,\n",
      " 'eval_meteor': 0.6088466325807146,\n",
      " 'eval_model_preparation_time': 0.003,\n",
      " 'eval_rouge1': 0.6654090098650332,\n",
      " 'eval_rouge2': 0.4747695313409016,\n",
      " 'eval_rougeL': 0.6229810019071551,\n",
      " 'eval_runtime': 749.3046,\n",
      " 'eval_samples_per_second': 4.671,\n",
      " 'eval_steps_per_second': 0.585}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Modèle de base (non fine-tuné)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "model_base.config.no_repeat_ngram_size = 3\n",
    "\n",
    "data_collator_base = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_base,\n",
    ")\n",
    "\n",
    "eval_args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"baseline_t5_base_en_fr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps = 2, \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"logs_baseline\",\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_base,\n",
    "    args=eval_args_base,\n",
    "    eval_dataset=tokenized_val,  # même val que pour LoRA\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_base,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    ")\n",
    "\n",
    "baseline_metrics = trainer_base.evaluate()\n",
    "print(\"Baseline metrics :\")\n",
    "pprint.pprint(baseline_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2a6aa-cf32-4f4b-b32c-19f3e624fbcd",
   "metadata": {},
   "source": [
    "Ici on crée un callback qui va être appelé à la fin de chaque epoch et qui permettra de vider la mémoire inutilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80756e91-3ae7-49f4-9b63-e4d950782b7f",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea8d29-935b-4cd6-aa92-626d13c246b9",
   "metadata": {},
   "source": [
    "On crée ici une fonction d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# per_device_eval_batch_size=8 \n",
    "# gradient_accumulation_steps = 2 réduire la mémoire : stabilité d’un batch effectif de 16, mais avec la mémoire d’un batch de 8 seulement, mais 2 fois plus long\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_t5_base_en_fr\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps = 2, \n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "\n",
    "        generation_max_length=128,\n",
    "        generation_num_beams=4,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36798a0-a59f-4722-b7bb-d59a63a28db4",
   "metadata": {},
   "source": [
    "## Entrainement du modèle fine-tuné"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c788e-f67b-40cf-a380-dd90d9128278",
   "metadata": {},
   "source": [
    "On entraine notre modèle et on l'évalue avec notre jeu de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79af0475-70c3-4909-9e73-9c37d2915e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_38968\\2911972018.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17726' max='87500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17726/87500 1:18:36 < 5:09:26, 3.76 it/s, Epoch 0.41/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Entraîner le modèle LoRA\n",
    "trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n",
    "\n",
    "# 2) Évaluer proprement le modèle LoRA sur le même jeu de validation\n",
    "lora_metrics = trainer.evaluate(eval_dataset=tokenized_val)\n",
    "print(\"Métriques modèle de base :\")\n",
    "pprint.pprint(baseline_metrics)\n",
    "print(\"Métriques modèle LoRA :\")\n",
    "pprint.pprint(lora_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac8f75-50f8-40f5-9183-9dd77b8ca0ee",
   "metadata": {},
   "source": [
    "On montre l'évolution des métriques au fur et à mesure de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9fbcc92-f4eb-43ec-95fe-050c3fadca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch   step  eval_loss  eval_bleu  eval_meteor  eval_rougeL\n",
      "875     1.0  43750   0.684410  40.336185     0.629312      0.64225\n",
      "1751    2.0  87500   0.683656  40.330786     0.629337      0.64239\n",
      "1753    2.0  87500   0.684410  40.336185     0.629312      0.64225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BASE_MODEL_NAME = \"t5-base\"\n",
    "PEFT_DIR = \"./finetuned_t5_base_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA et de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle de base + modèle LoRA rechargés depuis les chemins.\n"
     ]
    }
   ],
   "source": [
    "# 1) Tokenizer commun (T5-base) : on le charge \"UNE\" seule fois (sauf si vous faites un run de tout) :)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# 2) Modèle de base (non fine-tuné)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME).to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "# 3) Modèle LoRA fine-tuné (chargé depuis le dossier PEFT_DIR)\n",
    "lora_backbone = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    lora_backbone,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ").to(DEVICE)\n",
    "lora_model.eval()\n",
    "\n",
    "print(\"Modèle de base + modèle LoRA rechargés depuis les chemins.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256, num_beams=4):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=3,  # comme dans model_base.config\n",
    "        )\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f85a61a-431c-43ae-84b6-0f42ebda2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple train 0 ---\n",
      "EN      : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR gold : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "FR base : En tant qu'Européens, avec notre expérience, notre culture de paix et nos possibilités économiques, nous sommes également appelés à contribuer à un avenir meilleur pour l'Iraq.\n",
      "FR LoRA : En tant qu'Européens, avec notre expérience, notre culture de paix et nos perspectives économiques, nous sommes également appelés à contribuer à un avenir meilleur pour l'Irak.\n",
      "\n",
      "--- Exemple train 1 ---\n",
      "EN      : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR gold : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "FR base : Il est en effet clair que ceux qui se trouvent à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "FR LoRA : Il est en effet clair que ceux qui se trouvent à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "\n",
      "--- Exemple train 2 ---\n",
      "EN      : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR gold : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR base : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR LoRA : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset train\n",
    "for i in range(3):\n",
    "    en = train_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = train_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer) # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple train\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf1f59c-975b-4b20-b3ac-db71914d8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple val 0 ---\n",
      "EN      : Do you accept, Commissioner, that what we are facing in our single market is a double distortion, a distortion between beer and wine, which is grossly unfair, and the distortion arises that because of grossly different rates of taxation and excise duty between Member States.\n",
      "FR gold : (EN) Le commissaire admet-il que dans notre marché unique, nous sommes confrontés à une double inégalité, à savoir une inégalité entre la bière et le vin, ce qui est extrêmement injuste, et une inégalité imputable à la grande différence entre les taux d'imposition et les droits d'accises d'un État membre à l'autre.\n",
      "FR base : Acceptez-vous, Monsieur le Commissaire, que ce que nous faisons dans notre marché unique est une double distorsions, une distorsion entre la bière et le vin, qui est grossièrement injuste, en raison de taux d'imposition ou d’accises nettement différents entre les États membres?\n",
      "FR LoRA : Acceptez-vous, Monsieur le Commissaire, que ce que nous avons à faire dans notre marché unique est une double distorsions, une distorsion entre la bière et le vin, qui est grossièrement injuste, en raison de taux d'imposition ou accises nettement différents entre les États membres?\n",
      "\n",
      "--- Exemple val 1 ---\n",
      "EN      : To come back to my main point, I think that the Schengen Executive Committee has therefore been replaced by the Council.\n",
      "FR gold : En fermant cette parenthèse, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "FR base : Pour revenir à mon point principal, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "FR LoRA : Pour revenir à mon point principal, je pense que le comité exécutif Schengen a donc été remplacé par le Conseil.\n",
      "\n",
      "--- Exemple val 2 ---\n",
      "EN      : Naturally, a temporary Commission would only operate until the end of the year, when the new Parliament will carefully consider the choice of Commissioners for the next five-year term.\n",
      "FR gold : Naturellement, la Commission provisoire ne travaillera que jusqu'à la fin de l'année où le nouveau Parlement choisira une Commission pour les cinq années suivantes.\n",
      "FR base : Naturellement, une Commission temporaire ne fonctionnerait qu'à la fin de l'année, lorsque le nouveau Parlement examinera attentivement le choix des commissaires pour le prochain mandat de cinq ans.\n",
      "FR LoRA : Naturellement, une Commission temporaire ne fonctionnerait qu'à la fin de l'année, lorsque le nouveau Parlement examinera attentivement le choix des commissaires pour le prochain mandat de cinq ans.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset validation\n",
    "for i in range(3):\n",
    "    en = val_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = val_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer)  # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple val\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b53398-1fdb-49b0-907c-fc4c86aac63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN      : Hello, how are you?\n",
      "FR base : Bonjour, comment êtes-vous?\n",
      "FR LoRA : Bonjour, comment êtes-vous?\n",
      "\n",
      "EN      : This project is about automatic translation.\n",
      "FR base : Ce projet concerne la traduction automatique.\n",
      "FR LoRA : Ce projet concerne la traduction automatique.\n",
      "\n",
      "EN      : The weather is nice today.\n",
      "FR base : Le temps est agréable aujourd'hui.\n",
      "FR LoRA : Le temps est agréable aujourd'hui.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phrases de test perso\n",
    "tests = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"EN      :\", s)\n",
    "    print(\"FR base :\", translate_sentence(s, base_model, tokenizer))\n",
    "    print(\"FR LoRA :\", translate_sentence(s, lora_model, tokenizer))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

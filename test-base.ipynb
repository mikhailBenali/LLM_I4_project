{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89d9f-b58b-45c5-a708-57a16e6c7282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5666b6-f2e7-47e2-b14d-7c2a99737eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb6a9f41-64d6-4976-a9fc-b9e00aa3c7f9",
   "metadata": {},
   "source": [
    "## Membres du Groupe de Projet\n",
    "\n",
    "- Maxence KAMIONKA\n",
    "- Mikhaïl BENALI\n",
    "- Hadja BAH\n",
    "- Emmanuel DAGNOGO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd78f78-71ef-42b0-ac1c-869ca9b16332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd097f8d-3273-4d78-a673-e8a66b6308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b93c75-379a-4be3-821c-50c8a02598c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Libérer la RAM Python\n",
    "gc.collect()\n",
    "# Libérer la mémoire GPU inutilisée\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f9ae-8932-4f97-b23c-ce6df94c4421",
   "metadata": {},
   "source": [
    "## Test du modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e092c15-9e28-428f-85b2-c5e1c5ab8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"t5-base\"\n",
    "INSTRUCTION = \"translate English to French: \"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7e6434-1fcb-4810-9a7c-46782fb9850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7ed64-717b-4ff0-b732-dda9fd992d4c",
   "metadata": {},
   "source": [
    "## Utilisation de LoRa (Low-Rank Adaption)\n",
    "\n",
    "On utilise LoRA afin d'entrainer des paramètres extra tout en gelant ceux du modèle de base pour ne pas lui faire oublier ce qu'il sait déjà faire originellement. On tune alors ces poids supplémentaires plutôt que tout le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d372ab3b-4cec-44a2-8c4f-d9ea73344231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "def load_lora_model():\n",
    "    # on réutilise MODEL_NAME & tokenizer définis plus haut\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(DEVICE)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a060-ecd2-46f2-b60b-a5d76e048fe1",
   "metadata": {},
   "source": [
    "### Choix des jeux de données\n",
    "\n",
    "OPUS avait été considéré plus tôt mais les traductions un peu \"étranges\" remarquées ont fait que le dataset Europarl a été privilégié à la fin.\n",
    "\n",
    "\n",
    "**Europarl en-fr** : phrases longues, style plus formel, discours parlementaires -> plus de contexte par phrase.\n",
    "\n",
    "\n",
    "**Ajouter les TED Talks plus tard pour le langage oral, plus “conversational”, avec un ton explicatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa7e90-74c1-4029-af04-702f1fc3ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f9fdb7-00c5-4639-95a9-edcd31e004f6",
   "metadata": {},
   "source": [
    "### Charger et échantilloner le dataset OPUS et Europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02bdb58-a4d7-46da-b3b4-26366fd1f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_data_old(sample_size_train=80000, sample_size_val=1000):\n",
    "\n",
    "    ### OPUS ###\n",
    "    opus = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "    opus_train = opus[\"train\"].shuffle(seed=42).select(range(sample_size_train))\n",
    "    opus_val = opus[\"validation\"].shuffle(seed=42).select(range(sample_size_val))\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "\n",
    "    ### FUSION des 2 datasets ###\n",
    "    train_ds = concatenate_datasets([opus_train, euro_train])\n",
    "    val_ds = concatenate_datasets([opus_val, euro_val])\n",
    "    \n",
    "    # re-shuffle global\n",
    "    train_ds = train_ds.shuffle(seed=43)\n",
    "    val_ds   = val_ds.shuffle(seed=43)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def load_data(sample_size_train=5000, sample_size_val=200):\n",
    "\n",
    "    ### EUROPARL EN-FR ###\n",
    "    europarl = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", split=\"train\").shuffle(seed=42)\n",
    "    euro_train = europarl.select(range(sample_size_train))\n",
    "    euro_val = europarl.select(range(sample_size_train, sample_size_train + sample_size_val))\n",
    "    \n",
    "    return euro_train, euro_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc8a8b8-0059-4153-8860-f2245a78c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train example 0 ---\n",
      "EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "--- train example 1 ---\n",
      "EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "--- train example 2 ---\n",
      "EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "--- val example 0 ---\n",
      "EN : On behalf of my group, I would encourage them to keep to this rule.\n",
      "FR : Au nom de mon groupe, je l’encouragerai à poursuivre dans cette voie.\n",
      "\n",
      "--- val example 1 ---\n",
      "EN : Madam President, unemployment is the European Union's number one problem.\n",
      "FR : Madame le Président, dans l'Union européenne le chômage est le problème central.\n",
      "\n",
      "--- val example 2 ---\n",
      "EN : The time when political movements, pressure groups and governments denied the existence of environmental problems is over.\n",
      "FR : . (NL) Le temps où les courants politiques, les groupements d'intérêts et les autorités niaient l'existence des problèmes écologiques est révolu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données brutes\n",
    "train_ds, val_ds = load_data()\n",
    "\n",
    "def show_raw_example(ds, idx=0, prefix=\"train\"):\n",
    "    ex = ds[idx]\n",
    "    print(f\"--- {prefix} example {idx} ---\")\n",
    "    print(\"EN :\", ex[\"translation\"][\"en\"])\n",
    "    print(\"FR :\", ex[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "# 3 exemples du train et 3 de la val\n",
    "for i in range(3):\n",
    "    show_raw_example(train_ds, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_raw_example(val_ds, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58cd32-e203-4647-97cb-3468f07b3102",
   "metadata": {},
   "source": [
    "### Encoder l’anglais comme input et le français comme labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e919b3af-59f2-4a4f-b273-cac1bad260a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [INSTRUCTION + ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # Encodage des entrées\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encodage des cibles\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # ignorer les PAD dans la loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != pad_token_id else -100) for tok in seq]\n",
    "        for seq in label_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26dc39a0-4ada-4e0e-ab43-57db5f58e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(train, val, tokenizer):\n",
    "    tokenized_train = train.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train.column_names,\n",
    "    )\n",
    "    tokenized_val = val.map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=val.column_names,\n",
    "    )\n",
    "    return tokenized_train, tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ffe0be-9733-4bcd-b503-e03520f3c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 223,788,288 || trainable%: 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                              | 0/200 [00:00<?, ? examples/s]C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4271.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Charger modèle LoRA & réutiliser le tokenizer global\n",
    "model = load_lora_model()\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "tokenized_train, tokenized_val = tokenize_datasets(train_ds, val_ds, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f5ed41-0751-4860-aabb-b62d122c6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_example(raw_ds, tokenized_ds, idx=0, prefix=\"train\"):\n",
    "    raw = raw_ds[idx]\n",
    "    tok = tokenized_ds[idx]\n",
    "\n",
    "    print(f\"=== {prefix} example {idx} ===\")\n",
    "    print(\"RAW EN :\", raw[\"translation\"][\"en\"])\n",
    "    print(\"RAW FR :\", raw[\"translation\"][\"fr\"])\n",
    "    print()\n",
    "\n",
    "    # Inputs\n",
    "    print(\"input_ids[:20] :\", tok[\"input_ids\"][:20])\n",
    "    print(\"Decoded input  :\", tokenizer.decode(tok[\"input_ids\"], skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "    # Labels (enlevant les -100 pour re-décoder)\n",
    "    labels = tok[\"labels\"]\n",
    "    # On remplace les -100 par pad_token_id pour pouvoir décoder\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels_for_decode = [pad_id if x == -100 else x for x in labels]\n",
    "    print(\"labels[:20]     :\", labels[:20])\n",
    "    print(\"Decoded labels  :\", tokenizer.decode(labels_for_decode, skip_special_tokens=True))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1ad48a-590c-4800-b266-50f560637202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train example 0 ===\n",
      "RAW EN : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "RAW FR : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 282, 1611, 7, 6, 28, 69, 351, 6, 69, 1543, 13, 3065, 11, 69, 1456]\n",
      "Decoded input  : translate English to French: As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "\n",
      "labels[:20]     : [622, 2430, 3890, 35, 7, 238, 678, 7056, 6, 393, 1089, 11183, 6, 1089, 1543, 20, 50, 25060, 3, 15]\n",
      "Decoded labels  : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "\n",
      "=== train example 1 ===\n",
      "RAW EN : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "RAW FR : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 94, 405, 5071, 2516, 21, 1402, 24, 273, 113, 33, 300, 8, 3, 22469, 953]\n",
      "Decoded input  : translate English to French: It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "\n",
      "labels[:20]     : [802, 259, 3, 26, 31, 6752, 29500, 238, 110, 3787, 285, 21442, 29, 17, 3, 8934, 20, 50, 953, 20]\n",
      "Decoded labels  : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "\n",
      "=== train example 2 ===\n",
      "RAW EN : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "RAW FR : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 101, 17516, 857, 24, 8, 12326, 19, 1345, 1879, 250, 34, 11708, 7, 3, 9]\n",
      "Decoded input  : translate English to French: We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "\n",
      "labels[:20]     : [3, 18990, 6, 678, 12182, 106, 7, 238, 90, 29296, 259, 247, 3, 40, 22, 7408, 19185, 7, 288, 6]\n",
      "Decoded labels  : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n",
      "=== val example 0 ===\n",
      "RAW EN : On behalf of my group, I would encourage them to keep to this rule.\n",
      "RAW FR : Au nom de mon groupe, je l’encouragerai à poursuivre dans cette voie.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 461, 6089, 13, 82, 563, 6, 27, 133, 2454, 135, 12, 453, 12, 48, 3356]\n",
      "Decoded input  : translate English to French: On behalf of my group, I would encourage them to keep to this rule.\n",
      "\n",
      "labels[:20]     : [1957, 5252, 20, 1911, 4986, 6, 528, 3, 40, 22, 35, 3422, 4843, 7253, 3, 85, 20745, 60, 247, 3]\n",
      "Decoded labels  : Au nom de mon groupe, je l’encouragerai à poursuivre dans cette voie.\n",
      "\n",
      "=== val example 1 ===\n",
      "RAW EN : Madam President, unemployment is the European Union's number one problem.\n",
      "RAW FR : Madame le Président, dans l'Union européenne le chômage est le problème central.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 5428, 265, 1661, 6, 17646, 19, 8, 1611, 3545, 31, 7, 381, 80, 682, 5]\n",
      "Decoded input  : translate English to French: Madam President, unemployment is the European Union's number one problem.\n",
      "\n",
      "labels[:20]     : [27328, 90, 22971, 6, 247, 3, 40, 31, 19011, 15851, 90, 3, 524, 10079, 51, 545, 259, 90, 9472, 2069]\n",
      "Decoded labels  : Madame le Président, dans l'Union européenne le chômage est le problème central.\n",
      "\n",
      "=== val example 2 ===\n",
      "RAW EN : The time when political movements, pressure groups and governments denied the existence of environmental problems is over.\n",
      "RAW FR : . (NL) Le temps où les courants politiques, les groupements d'intérêts et les autorités niaient l'existence des problèmes écologiques est révolu.\n",
      "\n",
      "input_ids[:20] : [13959, 1566, 12, 2379, 10, 37, 97, 116, 1827, 9780, 6, 1666, 1637, 11, 10524, 11958, 8, 6831, 13, 3262]\n",
      "Decoded input  : translate English to French: The time when political movements, pressure groups and governments denied the existence of environmental problems is over.\n",
      "\n",
      "labels[:20]     : [3, 5, 41, 18207, 61, 312, 1993, 3, 2355, 110, 16203, 7, 16876, 6, 110, 563, 10420, 3, 26, 31]\n",
      "Decoded labels  : . (NL) Le temps où les courants politiques, les groupements d'intérêts et les autorités niaient l'existence des problèmes écologiques est révolu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_processed_example(train_ds, tokenized_train, i, prefix=\"train\")\n",
    "\n",
    "for i in range(3):\n",
    "    show_processed_example(val_ds, tokenized_val, i, prefix=\"val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05420-c5b9-4c1e-a4aa-5f23edb44f8e",
   "metadata": {},
   "source": [
    "### Métrique d'évaluation de la traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dac1a62-940d-48cf-bf27-e0f3e1acabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "## Pour nettoyer le texte\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    # eval_preds peut être un tuple (preds, labels)\n",
    "    # ou un objet EvalPrediction avec .predictions et .label_ids\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "    # Certains modèles renvoient (logits, ...) -> on garde seulement le 1er élément\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # On met tout en np.array pour être tranquille\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    # Cas où preds = logits (batch, seq_len, vocab_size) -> on prend l'argmax\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # On s'assure que ce sont bien des entiers\n",
    "    preds = preds.astype(\"int64\")\n",
    "\n",
    "    # si jamais il y a des valeurs négatives dans preds on les remplace par pad_token_id avant decode\n",
    "    preds[preds < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Gestion des labels : on remet pad_token_id à la place des -100 pour décoder\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode des prédictions et des labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage simple\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # sacreBLEU / METEOR / ROUGE\n",
    "    # sacreBLEU attend une liste de listes pour les références\n",
    "    refs_list = [[r] for r in decoded_labels]\n",
    "\n",
    "    bleu_res = sacrebleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=refs_list,\n",
    "    )\n",
    "\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    meteor_res = meteor.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_res[\"score\"],\n",
    "        \"meteor\": meteor_res[\"meteor\"],\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33188-476e-43b6-81a8-34394241864b",
   "metadata": {},
   "source": [
    "## Métriques sur modèle de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b37d9e-9dea-4adc-8bbf-5e6ce0783d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_12832\\3450106949.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Seq2SeqTrainer(\n",
      "C:\\Users\\maxka\\anaconda3\\envs\\llm_trad\\Lib\\site-packages\\transformers\\generation\\utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics : {'eval_loss': 1.0082529783248901, 'eval_model_preparation_time': 0.003, 'eval_bleu': 34.32950213928655, 'eval_meteor': 0.6026120330172933, 'eval_rouge1': 0.6638890688050059, 'eval_rouge2': 0.45996260002474126, 'eval_rougeL': 0.620193619313274, 'eval_runtime': 45.4089, 'eval_samples_per_second': 4.404, 'eval_steps_per_second': 0.551}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Modèle de base (non fine-tuné)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "model_base.config.no_repeat_ngram_size = 3\n",
    "\n",
    "# On réutilise le même val set tokenisé\n",
    "data_collator_base = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_base,\n",
    ")\n",
    "\n",
    "eval_args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"baseline_t5_base_en_fr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"logs_baseline\",\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_base,\n",
    "    args=eval_args_base,\n",
    "    eval_dataset=tokenized_val,  # même val que pour LoRA\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_base,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    ")\n",
    "\n",
    "baseline_metrics = trainer_base.evaluate()\n",
    "print(\"Baseline metrics :\", baseline_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4ab88b-7b31-4ba8-bba5-bc16fd3e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GarbageCollectorCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Libérer la RAM Python\n",
    "        gc.collect()\n",
    "        # Libérer la mémoire GPU inutilisée\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80756e91-3ae7-49f4-9b63-e4d950782b7f",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08124388-0e36-42f4-bdaa-d1df092dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def train(model, tokenizer, tokenized_train, tokenized_val):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"finetuned_t5_base_en_fr\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bleu\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "\n",
    "        generation_max_length=128,\n",
    "        generation_num_beams=4,\n",
    "    )\n",
    "\n",
    "    gc_callback = GarbageCollectorCallback()\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[gc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    print(\"Best eval :\", trainer.state.best_metric)\n",
    "    print(\"Best checkpoint :\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36798a0-a59f-4722-b7bb-d59a63a28db4",
   "metadata": {},
   "source": [
    "## Entrainement du modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79af0475-70c3-4909-9e73-9c37d2915e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxka\\AppData\\Local\\Temp\\ipykernel_12832\\302199612.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 06:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.792700</td>\n",
       "      <td>0.767098</td>\n",
       "      <td>36.737944</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>0.673695</td>\n",
       "      <td>0.475615</td>\n",
       "      <td>0.631384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.764936</td>\n",
       "      <td>36.717175</td>\n",
       "      <td>0.614851</td>\n",
       "      <td>0.674162</td>\n",
       "      <td>0.474805</td>\n",
       "      <td>0.631212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.737900</td>\n",
       "      <td>0.764149</td>\n",
       "      <td>36.697760</td>\n",
       "      <td>0.614943</td>\n",
       "      <td>0.673988</td>\n",
       "      <td>0.474629</td>\n",
       "      <td>0.631332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eval : 36.73794379438751\n",
      "Best checkpoint : finetuned_t5_base_en_fr\\checkpoint-625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métriques modèle de base : {'eval_loss': 1.0082529783248901, 'eval_model_preparation_time': 0.003, 'eval_bleu': 34.32950213928655, 'eval_meteor': 0.6026120330172933, 'eval_rouge1': 0.6638890688050059, 'eval_rouge2': 0.45996260002474126, 'eval_rougeL': 0.620193619313274, 'eval_runtime': 45.4089, 'eval_samples_per_second': 4.404, 'eval_steps_per_second': 0.551}\n",
      "Métriques modèle LoRA    : {'eval_loss': 0.7670983672142029, 'eval_bleu': 36.73794379438751, 'eval_meteor': 0.6138114364037918, 'eval_rouge1': 0.673695196927682, 'eval_rouge2': 0.475615149964153, 'eval_rougeL': 0.6313841125429722, 'eval_runtime': 46.3403, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 0.539, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# 1) Entraîner le modèle LoRA\n",
    "trainer = train(model, tokenizer, tokenized_train, tokenized_val)\n",
    "\n",
    "# 2) Évaluer proprement le modèle LoRA sur le même jeu de validation\n",
    "lora_metrics = trainer.evaluate(eval_dataset=tokenized_val)\n",
    "print(\"Métriques modèle de base :\", baseline_metrics)\n",
    "print(\"Métriques modèle LoRA    :\", lora_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9fbcc92-f4eb-43ec-95fe-050c3fadca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  step  eval_loss  eval_bleu  eval_meteor  eval_rougeL\n",
      "12    1.0   625   0.767098  36.737944     0.613811     0.631384\n",
      "26    2.0  1250   0.764936  36.717175     0.614851     0.631212\n",
      "39    3.0  1875   0.764149  36.697760     0.614943     0.631332\n",
      "41    3.0  1875   0.767098  36.737944     0.613811     0.631384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Lignes d'éval (celles qui ont une eval_loss)\n",
    "eval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "# Colonnes qui nous intéressent\n",
    "cols = [\"epoch\", \"step\", \"eval_loss\", \"eval_bleu\", \"eval_meteor\", \"eval_rougeL\"]\n",
    "\n",
    "print(eval_logs[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909fc78-33d2-4c16-bb34-a3bb8a073ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f044012-f22e-4e3e-bbe7-201b29bc4aaf",
   "metadata": {},
   "source": [
    "# Recharger le modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb680ca-74fa-4f8d-8943-e1b462240f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BASE_MODEL_NAME = \"t5-base\"\n",
    "PEFT_DIR = \"./finetuned_t5_base_en_fr\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16356156-9123-46d2-a51a-cb7fade47d54",
   "metadata": {},
   "source": [
    "## Charger le modèle LoRA et de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac4640a-ebdb-44eb-8d88-c6219150a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle de base + modèle LoRA rechargés depuis les chemins.\n"
     ]
    }
   ],
   "source": [
    "# 1) Tokenizer commun (T5-base) : on le charge UNE seule fois\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# 2) Modèle de base (non fine-tuné)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME).to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "# 3) Modèle LoRA fine-tuné (chargé depuis le dossier PEFT_DIR)\n",
    "lora_backbone = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    lora_backbone,\n",
    "    PEFT_DIR,\n",
    "    local_files_only=True,\n",
    ").to(DEVICE)\n",
    "lora_model.eval()\n",
    "\n",
    "print(\"Modèle de base + modèle LoRA rechargés depuis les chemins.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5329-0195-4f85-88a3-4c424a484f9b",
   "metadata": {},
   "source": [
    "## Fonction de traduction + tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bba25ba-f52b-47cc-a15c-59505d3da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"translate English to French: \"\n",
    "\n",
    "def translate_sentence(sentence, model, tokenizer, max_length=256, num_beams=4):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        INSTRUCTION + sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=3,  # comme dans model_base.config\n",
    "        )\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f85a61a-431c-43ae-84b6-0f42ebda2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple train 0 ---\n",
      "EN      : As Europeans, with our experience, our culture of peace and our economic opportunities, we too are called upon to make our contribution towards a better future for Iraq.\n",
      "FR gold : Les Européens que nous sommes, avec leur expérience, leur culture de la paix et leurs moyens économiques, sont appelés à apporter leur contribution en faveur d'un avenir meilleur en Irak.\n",
      "FR base : En tant qu'Européens, avec notre expérience, notre culture de paix et nos possibilités économiques, nous sommes également appelés à contribuer à un avenir meilleur pour l'Iraq.\n",
      "FR LoRA : En tant qu'Européens, avec notre expérience, notre culture de paix et nos perspectives économiques, nous sommes également appelés à contribuer à un meilleur avenir pour l'Irak.\n",
      "\n",
      "--- Exemple train 1 ---\n",
      "EN      : It does indeed speak for itself that those who are around the negotiating table are most sensitive to their own issues; this is always the case.\n",
      "FR gold : Il est d'ailleurs évident que les personnes qui siègent autour de la table de négociations sont les plus sensibles à leurs propres problèmes. Il en est toujours ainsi.\n",
      "FR base : Il est en effet clair que ceux qui se trouvent à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "FR LoRA : Il est en effet clair que ceux qui sont à la table des négociations sont les plus sensibles à leurs propres questions; c'est toujours le cas.\n",
      "\n",
      "--- Exemple train 2 ---\n",
      "EN      : We nevertheless believe that the compromise is sound overall because it constitutes a clear improvement upon the original proposal.\n",
      "FR gold : Cependant, nous estimons que le compromis est dans l’ensemble satisfaisant, en ce sens qu’il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR base : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "FR LoRA : Nous pensons néanmoins que le compromis est globalement sain, car il constitue une nette amélioration par rapport à la proposition initiale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset train\n",
    "for i in range(3):\n",
    "    en = train_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = train_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer) # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple train\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf1f59c-975b-4b20-b3ac-db71914d8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exemple val 0 ---\n",
      "EN      : On behalf of my group, I would encourage them to keep to this rule.\n",
      "FR gold : Au nom de mon groupe, je l’encouragerai à poursuivre dans cette voie.\n",
      "FR base : Au nom de mon groupe, je les encourage à respecter cette règle.\n",
      "FR LoRA : Au nom de mon groupe, je les encourage à respecter cette règle.\n",
      "\n",
      "--- Exemple val 1 ---\n",
      "EN      : Madam President, unemployment is the European Union's number one problem.\n",
      "FR gold : Madame le Président, dans l'Union européenne le chômage est le problème central.\n",
      "FR base : Madame le Président, le chômage est le principal problème de l'Union européenne.\n",
      "FR LoRA : Madame le Président, le chômage est le problème numéro un de l'Union européenne.\n",
      "\n",
      "--- Exemple val 2 ---\n",
      "EN      : The time when political movements, pressure groups and governments denied the existence of environmental problems is over.\n",
      "FR gold : . (NL) Le temps où les courants politiques, les groupements d'intérêts et les autorités niaient l'existence des problèmes écologiques est révolu.\n",
      "FR base : L'époque où les mouvements politiques, les groupes de pression et les gouvernements ont nié l'existence de problèmes environnementaux est terminée.\n",
      "FR LoRA : L'époque où les mouvements politiques, les groupes de pression et les gouvernements ont nié l'existence de problèmes environnementaux est terminée.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quelques phrases du dataset validation\n",
    "for i in range(3):\n",
    "    en = val_ds[i][\"translation\"][\"en\"]\n",
    "    fr_gold = val_ds[i][\"translation\"][\"fr\"]\n",
    "\n",
    "    fr_base = translate_sentence(en, base_model, tokenizer)\n",
    "    fr_lora = translate_sentence(en, lora_model, tokenizer)  # modèle fine-tuné LoRA\n",
    "\n",
    "    print(\"--- Exemple val\", i, \"---\")\n",
    "    print(\"EN      :\", en)\n",
    "    print(\"FR gold :\", fr_gold)\n",
    "    print(\"FR base :\", fr_base)\n",
    "    print(\"FR LoRA :\", fr_lora)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7b53398-1fdb-49b0-907c-fc4c86aac63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN      : Hello, how are you?\n",
      "FR base : Bonjour, comment êtes-vous?\n",
      "FR LoRA : Bonjour, comment êtes-vous?\n",
      "\n",
      "EN      : This project is about automatic translation.\n",
      "FR base : Ce projet concerne la traduction automatique.\n",
      "FR LoRA : Ce projet concerne la traduction automatique.\n",
      "\n",
      "EN      : The weather is nice today.\n",
      "FR base : Le temps est agréable aujourd'hui.\n",
      "FR LoRA : Le temps est agréable aujourd'hui.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phrases de test perso\n",
    "tests = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This project is about automatic translation.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"EN      :\", s)\n",
    "    print(\"FR base :\", translate_sentence(s, base_model, tokenizer))\n",
    "    print(\"FR LoRA :\", translate_sentence(s, lora_model, tokenizer))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a19ce1-2ef5-428e-b517-b9defa02cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
